
@misc{noauthor_computers_nodate,
	title = {Computers ace {IQ} tests but still make dumb mistakes. {Can} different tests help?},
	url = {https://www.science.org/content/article/computers-ace-iq-tests-still-make-dumb-mistakes-can-different-tests-help},
	abstract = {AI researchers are creating novel “benchmarks” to help models avoid real-world stumbles},
	language = {en},
	urldate = {2024-02-26},
}

@book{wooldridge_brief_2021,
	title = {A {Brief} {History} of {Artificial} {Intelligence}: {What} {It} {Is}, {Where} {We} {Are}, and {Where} {We} {Are} {Going}},
	isbn = {978-1-250-77073-8},
	url = {https://books.google.co.uk/books?id=5MDiDwAAQBAJ},
	publisher = {Flatiron Books},
	author = {Wooldridge, M.},
	year = {2021},
}

@misc{noauthor_how_nodate,
	title = {How fast is {AI} improving?},
	url = {https://theaidigest.org/},
	abstract = {A visual explainer on the rate and risks of AI progress},
	language = {en},
	urldate = {2024-02-26},
	journal = {AI Digest},
}

@misc{noauthor_-context_nodate,
	title = {In-context {Learning} and {Induction} {Heads}},
	url = {https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html},
	urldate = {2024-02-26},
}

@misc{noauthor_chapter_nodate,
	title = {Chapter 1 - {Capabilities} - [{Commentable}]},
	url = {https://docs.google.com/document/d/1HKo0Kest9Xppjn7m2ODpfMUlEu93SzLsfxXBH48Xaus/edit?usp=drive_web&ouid=112977021496180539506&usp=embed_facebook},
	language = {fr},
	urldate = {2024-02-26},
	journal = {Google Docs},
	file = {Snapshot:/home/jeanne/Zotero/storage/36DWFAT5/edit.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/E55HZ3NL/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/F7RQFR5S/2005.html:text/html},
}

@misc{kadavath_language_2022,
	title = {Language {Models} ({Mostly}) {Know} {What} {They} {Know}},
	url = {http://arxiv.org/abs/2207.05221},
	doi = {10.48550/arXiv.2207.05221},
	abstract = {We study whether language models can evaluate the validity of their own claims and predict which questions they will be able to answer correctly. We first show that larger models are well-calibrated on diverse multiple choice and true/false questions when they are provided in the right format. Thus we can approach self-evaluation on open-ended sampling tasks by asking models to first propose answers, and then to evaluate the probability "P(True)" that their answers are correct. We find encouraging performance, calibration, and scaling for P(True) on a diverse array of tasks. Performance at self-evaluation further improves when we allow models to consider many of their own samples before predicting the validity of one specific possibility. Next, we investigate whether models can be trained to predict "P(IK)", the probability that "I know" the answer to a question, without reference to any particular proposed answer. Models perform well at predicting P(IK) and partially generalize across tasks, though they struggle with calibration of P(IK) on new tasks. The predicted P(IK) probabilities also increase appropriately in the presence of relevant source materials in the context, and in the presence of hints towards the solution of mathematical word problems. We hope these observations lay the groundwork for training more honest models, and for investigating how honesty generalizes to cases where models are trained on objectives other than the imitation of human writing.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
	month = nov,
	year = {2022},
	note = {arXiv:2207.05221 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/XZQ3VUE5/Kadavath et al. - 2022 - Language Models (Mostly) Know What They Know.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/BB4IQKLL/2207.html:text/html},
}

@misc{kosinski_evaluating_2024,
	title = {Evaluating {Large} {Language} {Models} in {Theory} of {Mind} {Tasks}},
	url = {http://arxiv.org/abs/2302.02083},
	doi = {10.48550/arXiv.2302.02083},
	abstract = {Eleven Large Language Models (LLMs) were assessed using a custom-made battery of false-belief tasks, considered a gold standard in testing Theory of Mind (ToM) in humans. The battery included 640 prompts spread across 40 diverse tasks, each one including a false-belief scenario, three closely matched true-belief control scenarios, and the reversed versions of all four. To solve a single task, a model needed to correctly answer 16 prompts across all eight scenarios. Smaller and older models solved no tasks; GPT-3-davinci-003 (from November 2022) and ChatGPT-3.5-turbo (from March 2023) solved 20\% of the tasks; ChatGPT-4 (from June 2023) solved 75\% of the tasks, matching the performance of six-year-old children observed in past studies. We explore the potential interpretation of these findings, including the intriguing possibility that ToM, previously considered exclusive to humans, may have spontaneously emerged as a byproduct of LLMs' improving language skills.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Kosinski, Michal},
	month = feb,
	year = {2024},
	note = {arXiv:2302.02083 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Human-Computer Interaction},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/VC2B8GF9/Kosinski - 2024 - Evaluating Large Language Models in Theory of Mind.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/NLXSKXS6/2302.html:text/html},
}

@misc{xu_opentom_2024,
	title = {{OpenToM}: {A} {Comprehensive} {Benchmark} for {Evaluating} {Theory}-of-{Mind} {Reasoning} {Capabilities} of {Large} {Language} {Models}},
	shorttitle = {{OpenToM}},
	url = {http://arxiv.org/abs/2402.06044},
	doi = {10.48550/arXiv.2402.06044},
	abstract = {Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Xu, Hainiu and Zhao, Runcong and Zhu, Lixing and Du, Jinhua and He, Yulan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06044 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/P9UZULM3/Xu et al. - 2024 - OpenToM A Comprehensive Benchmark for Evaluating .pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/BSJJ6J6J/2402.html:text/html},
}

@misc{qin_toolllm_2023,
	title = {{ToolLLM}: {Facilitating} {Large} {Language} {Models} to {Master} 16000+ {Real}-world {APIs}},
	shorttitle = {{ToolLLM}},
	url = {http://arxiv.org/abs/2307.16789},
	doi = {10.48550/arXiv.2307.16789},
	abstract = {Despite the advancements of open-source large language models (LLMs), e.g., LLaMA, they remain significantly limited in tool-use capabilities, i.e., using external tools (APIs) to fulfill human instructions. The reason is that current instruction tuning largely focuses on basic language tasks but ignores the tool-use domain. This is in contrast to the excellent tool-use capabilities of state-of-the-art (SOTA) closed-source LLMs, e.g., ChatGPT. To bridge this gap, we introduce ToolLLM, a general tool-use framework encompassing data construction, model training, and evaluation. We first present ToolBench, an instruction-tuning dataset for tool use, which is constructed automatically using ChatGPT. Specifically, the construction can be divided into three stages: (i) API collection: we collect 16,464 real-world RESTful APIs spanning 49 categories from RapidAPI Hub; (ii) instruction generation: we prompt ChatGPT to generate diverse instructions involving these APIs, covering both single-tool and multi-tool scenarios; (iii) solution path annotation: we use ChatGPT to search for a valid solution path (chain of API calls) for each instruction. To enhance the reasoning capabilities of LLMs, we develop a novel depth-first search-based decision tree algorithm. It enables LLMs to evaluate multiple reasoning traces and expand the search space. Moreover, to evaluate the tool-use capabilities of LLMs, we develop an automatic evaluator: ToolEval. Based on ToolBench, we fine-tune LLaMA to obtain an LLM ToolLLaMA, and equip it with a neural API retriever to recommend appropriate APIs for each instruction. Experiments show that ToolLLaMA demonstrates a remarkable ability to execute complex instructions and generalize to unseen APIs, and exhibits comparable performance to ChatGPT. Our ToolLLaMA also demonstrates strong zero-shot generalization ability in an out-of-distribution tool-use dataset: APIBench.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and Zhao, Sihan and Hong, Lauren and Tian, Runchu and Xie, Ruobing and Zhou, Jie and Gerstein, Mark and Li, Dahai and Liu, Zhiyuan and Sun, Maosong},
	month = oct,
	year = {2023},
	note = {arXiv:2307.16789 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/KJPL7EMC/Qin et al. - 2023 - ToolLLM Facilitating Large Language Models to Mas.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/4P8TEFWZ/2307.html:text/html},
}

@misc{shinn_reflexion_2023,
	title = {Reflexion: {Language} {Agents} with {Verbal} {Reinforcement} {Learning}},
	shorttitle = {Reflexion},
	url = {http://arxiv.org/abs/2303.11366},
	doi = {10.48550/arXiv.2303.11366},
	abstract = {Large language models (LLMs) have been increasingly used to interact with external environments (e.g., games, compilers, APIs) as goal-driven agents. However, it remains challenging for these language agents to quickly and efficiently learn from trial-and-error as traditional reinforcement learning methods require extensive training samples and expensive model fine-tuning. We propose Reflexion, a novel framework to reinforce language agents not by updating weights, but instead through linguistic feedback. Concretely, Reflexion agents verbally reflect on task feedback signals, then maintain their own reflective text in an episodic memory buffer to induce better decision-making in subsequent trials. Reflexion is flexible enough to incorporate various types (scalar values or free-form language) and sources (external or internally simulated) of feedback signals, and obtains significant improvements over a baseline agent across diverse tasks (sequential decision-making, coding, language reasoning). For example, Reflexion achieves a 91\% pass@1 accuracy on the HumanEval coding benchmark, surpassing the previous state-of-the-art GPT-4 that achieves 80\%. We also conduct ablation and analysis studies using different feedback signals, feedback incorporation methods, and agent types, and provide insights into how they affect performance.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Shinn, Noah and Cassano, Federico and Berman, Edward and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
	month = oct,
	year = {2023},
	note = {arXiv:2303.11366 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/7GNHYICR/Shinn et al. - 2023 - Reflexion Language Agents with Verbal Reinforceme.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/4I7CPUCX/2303.html:text/html},
}

@misc{bubeck_sparks_2023,
	title = {Sparks of {Artificial} {General} {Intelligence}: {Early} experiments with {GPT}-4},
	shorttitle = {Sparks of {Artificial} {General} {Intelligence}},
	url = {http://arxiv.org/abs/2303.12712},
	doi = {10.48550/arXiv.2303.12712},
	abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Bubeck, Sébastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
	month = apr,
	year = {2023},
	note = {arXiv:2303.12712 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/3ZM8ZD56/Bubeck et al. - 2023 - Sparks of Artificial General Intelligence Early e.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/8NZLTD4G/2303.html:text/html},
}

@misc{noauthor_redirecting_nodate,
	title = {Redirecting},
	url = {https://www.google.com/url?q=https://www.nature.com/articles/s41586-023-06747-5&sa=D&source=docs&ust=1708981142950784&usg=AOvVaw21iYgCejcuo2fKB4A1cUQG},
	urldate = {2024-02-26},
}

@misc{noauthor_solving_nodate,
	title = {Solving olympiad geometry without human demonstrations {\textbar} {Nature}},
	url = {https://www.nature.com/articles/s41586-023-06747-5},
	urldate = {2024-02-26},
}

@misc{noauthor_gpt-4_nodate,
	title = {{GPT}-4},
	url = {https://openai.com/research/gpt-4},
	urldate = {2024-02-26},
}

@misc{noauthor_segment_nodate,
	title = {Segment {Anything} {Model} ({SAM}) - {The} {Complete} 2024 {Guide} - viso.ai},
	url = {https://viso.ai/deep-learning/segment-anything-model-sam-explained/},
	urldate = {2024-02-26},
}

@misc{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.48550/arXiv.1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv:1406.2661 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/PZEB3GQ7/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/ATEN2XYF/1406.html:text/html},
}

@misc{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	doi = {10.48550/arXiv.1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	month = jan,
	year = {2016},
	note = {arXiv:1511.06434 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/99WTHWD6/Radford et al. - 2016 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/3FW5GE9A/1511.html:text/html},
}

@misc{liu_coupled_2016,
	title = {Coupled {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1606.07536},
	doi = {10.48550/arXiv.1606.07536},
	abstract = {We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Liu, Ming-Yu and Tuzel, Oncel},
	month = sep,
	year = {2016},
	note = {arXiv:1606.07536 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/URKPYZ5G/Liu et Tuzel - 2016 - Coupled Generative Adversarial Networks.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/NVBFN5ZU/1606.html:text/html},
}

@misc{karras_progressive_2018,
	title = {Progressive {Growing} of {GANs} for {Improved} {Quality}, {Stability}, and {Variation}},
	url = {http://arxiv.org/abs/1710.10196},
	doi = {10.48550/arXiv.1710.10196},
	abstract = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024{\textasciicircum}2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
	month = feb,
	year = {2018},
	note = {arXiv:1710.10196 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/F46SKVZZ/Karras et al. - 2018 - Progressive Growing of GANs for Improved Quality, .pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/KX7MUTI4/1710.html:text/html},
}

@misc{noauthor_how_nodate-1,
	title = {How {Midjourney} {Evolved} {Over} {Time} ({Comparing} {V1} to {V6} {Outputs}) {\textbar} {Gold} {Penguin}},
	url = {https://goldpenguin.org/blog/midjourney-v1-to-v6-evolution/},
	urldate = {2024-02-26},
	file = {How Midjourney Evolved Over Time (Comparing V1 to V6 Outputs) | Gold Penguin:/home/jeanne/Zotero/storage/CE6REKNT/midjourney-v1-to-v6-evolution.html:text/html},
}

@misc{alayrac_flamingo_2022,
	title = {Flamingo: a {Visual} {Language} {Model} for {Few}-{Shot} {Learning}},
	shorttitle = {Flamingo},
	url = {http://arxiv.org/abs/2204.14198},
	doi = {10.48550/arXiv.2204.14198},
	abstract = {Building models that can be rapidly adapted to novel tasks using only a handful of annotated examples is an open challenge for multimodal machine learning research. We introduce Flamingo, a family of Visual Language Models (VLM) with this ability. We propose key architectural innovations to: (i) bridge powerful pretrained vision-only and language-only models, (ii) handle sequences of arbitrarily interleaved visual and textual data, and (iii) seamlessly ingest images or videos as inputs. Thanks to their flexibility, Flamingo models can be trained on large-scale multimodal web corpora containing arbitrarily interleaved text and images, which is key to endow them with in-context few-shot learning capabilities. We perform a thorough evaluation of our models, exploring and measuring their ability to rapidly adapt to a variety of image and video tasks. These include open-ended tasks such as visual question-answering, where the model is prompted with a question which it has to answer; captioning tasks, which evaluate the ability to describe a scene or an event; and close-ended tasks such as multiple-choice visual question-answering. For tasks lying anywhere on this spectrum, a single Flamingo model can achieve a new state of the art with few-shot learning, simply by prompting the model with task-specific examples. On numerous benchmarks, Flamingo outperforms models fine-tuned on thousands of times more task-specific data.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katie and Reynolds, Malcolm and Ring, Roman and Rutherford, Eliza and Cabi, Serkan and Han, Tengda and Gong, Zhitao and Samangooei, Sina and Monteiro, Marianne and Menick, Jacob and Borgeaud, Sebastian and Brock, Andrew and Nematzadeh, Aida and Sharifzadeh, Sahand and Binkowski, Mikolaj and Barreira, Ricardo and Vinyals, Oriol and Zisserman, Andrew and Simonyan, Karen},
	month = nov,
	year = {2022},
	note = {arXiv:2204.14198 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/PPMGGHNW/Alayrac et al. - 2022 - Flamingo a Visual Language Model for Few-Shot Lea.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/PX8VQLHV/2204.html:text/html},
}

@misc{reed_generalist_2022,
	title = {A {Generalist} {Agent}},
	url = {http://arxiv.org/abs/2205.06175},
	doi = {10.48550/arXiv.2205.06175},
	abstract = {Inspired by progress in large-scale language modeling, we apply a similar approach towards building a single generalist agent beyond the realm of text outputs. The agent, which we refer to as Gato, works as a multi-modal, multi-task, multi-embodiment generalist policy. The same network with the same weights can play Atari, caption images, chat, stack blocks with a real robot arm and much more, deciding based on its context whether to output text, joint torques, button presses, or other tokens. In this report we describe the model and the data, and document the current capabilities of Gato.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and Eccles, Tom and Bruce, Jake and Razavi, Ali and Edwards, Ashley and Heess, Nicolas and Chen, Yutian and Hadsell, Raia and Vinyals, Oriol and Bordbar, Mahyar and de Freitas, Nando},
	month = nov,
	year = {2022},
	note = {arXiv:2205.06175 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/K8JHG6ZL/Reed et al. - 2022 - A Generalist Agent.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/6VLALWEU/2205.html:text/html},
}

@misc{gemini_team_gemini_2023,
	title = {Gemini: {A} {Family} of {Highly} {Capable} {Multimodal} {Models}},
	shorttitle = {Gemini},
	url = {http://arxiv.org/abs/2312.11805},
	doi = {10.48550/arXiv.2312.11805},
	abstract = {This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of Gemini models in cross-modal reasoning and language understanding will enable a wide variety of use cases and we discuss our approach toward deploying them responsibly to users.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Gemini Team and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M. and Hauth, Anja and Millican, Katie and Silver, David and Petrov, Slav and Johnson, Melvin and Antonoglou, Ioannis and Schrittwieser, Julian and Glaese, Amelia and Chen, Jilin and Pitler, Emily and Lillicrap, Timothy and Lazaridou, Angeliki and Firat, Orhan and Molloy, James and Isard, Michael and Barham, Paul R. and Hennigan, Tom and Lee, Benjamin and Viola, Fabio and Reynolds, Malcolm and Xu, Yuanzhong and Doherty, Ryan and Collins, Eli and Meyer, Clemens and Rutherford, Eliza and Moreira, Erica and Ayoub, Kareem and Goel, Megha and Tucker, George and Piqueras, Enrique and Krikun, Maxim and Barr, Iain and Savinov, Nikolay and Danihelka, Ivo and Roelofs, Becca and White, Anaïs and Andreassen, Anders and von Glehn, Tamara and Yagati, Lakshman and Kazemi, Mehran and Gonzalez, Lucas and Khalman, Misha and Sygnowski, Jakub and Frechette, Alexandre and Smith, Charlotte and Culp, Laura and Proleev, Lev and Luan, Yi and Chen, Xi and Lottes, James and Schucher, Nathan and Lebron, Federico and Rrustemi, Alban and Clay, Natalie and Crone, Phil and Kocisky, Tomas and Zhao, Jeffrey and Perz, Bartek and Yu, Dian and Howard, Heidi and Bloniarz, Adam and Rae, Jack W. and Lu, Han and Sifre, Laurent and Maggioni, Marcello and Alcober, Fred and Garrette, Dan and Barnes, Megan and Thakoor, Shantanu and Austin, Jacob and Barth-Maron, Gabriel and Wong, William and Joshi, Rishabh and Chaabouni, Rahma and Fatiha, Deeni and Ahuja, Arun and Liu, Ruibo and Li, Yunxuan and Cogan, Sarah and Chen, Jeremy and Jia, Chao and Gu, Chenjie and Zhang, Qiao and Grimstad, Jordan and Hartman, Ale Jakse and Chadwick, Martin and Tomar, Gaurav Singh and Garcia, Xavier and Senter, Evan and Taropa, Emanuel and Pillai, Thanumalayan Sankaranarayana and Devlin, Jacob and Laskin, Michael and Casas, Diego de Las and Valter, Dasha and Tao, Connie and Blanco, Lorenzo and Badia, Adrià Puigdomènech and Reitter, David and Chen, Mianna and Brennan, Jenny and Rivera, Clara and Brin, Sergey and Iqbal, Shariq and Surita, Gabriela and Labanowski, Jane and Rao, Abhi and Winkler, Stephanie and Parisotto, Emilio and Gu, Yiming and Olszewska, Kate and Zhang, Yujing and Addanki, Ravi and Miech, Antoine and Louis, Annie and Shafey, Laurent El and Teplyashin, Denis and Brown, Geoff and Catt, Elliot and Attaluri, Nithya and Balaguer, Jan and Xiang, Jackie and Wang, Pidong and Ashwood, Zoe and Briukhov, Anton and Webson, Albert and Ganapathy, Sanjay and Sanghavi, Smit and Kannan, Ajay and Chang, Ming-Wei and Stjerngren, Axel and Djolonga, Josip and Sun, Yuting and Bapna, Ankur and Aitchison, Matthew and Pejman, Pedram and Michalewski, Henryk and Yu, Tianhe and Wang, Cindy and Love, Juliette and Ahn, Junwhan and Bloxwich, Dawn and Han, Kehang and Humphreys, Peter and Sellam, Thibault and Bradbury, James and Godbole, Varun and Samangooei, Sina and Damoc, Bogdan and Kaskasoli, Alex and Arnold, Sébastien M. R. and Vasudevan, Vijay and Agrawal, Shubham and Riesa, Jason and Lepikhin, Dmitry and Tanburn, Richard and Srinivasan, Srivatsan and Lim, Hyeontaek and Hodkinson, Sarah and Shyam, Pranav and Ferret, Johan and Hand, Steven and Garg, Ankush and Paine, Tom Le and Li, Jian and Li, Yujia and Giang, Minh and Neitz, Alexander and Abbas, Zaheer and York, Sarah and Reid, Machel and Cole, Elizabeth and Chowdhery, Aakanksha and Das, Dipanjan and Rogozińska, Dominika and Nikolaev, Vitaly and Sprechmann, Pablo and Nado, Zachary and Zilka, Lukas and Prost, Flavien and He, Luheng and Monteiro, Marianne and Mishra, Gaurav and Welty, Chris and Newlan, Josh and Jia, Dawei and Allamanis, Miltiadis and Hu, Clara Huiyi and de Liedekerke, Raoul and Gilmer, Justin and Saroufim, Carl and Rijhwani, Shruti and Hou, Shaobo and Shrivastava, Disha and Baddepudi, Anirudh and Goldin, Alex and Ozturel, Adnan and Cassirer, Albin and Xu, Yunhan and Sohn, Daniel and Sachan, Devendra and Amplayo, Reinald Kim and Swanson, Craig and Petrova, Dessie and Narayan, Shashi and Guez, Arthur and Brahma, Siddhartha and Landon, Jessica and Patel, Miteyan and Zhao, Ruizhe and Villela, Kevin and Wang, Luyu and Jia, Wenhao and Rahtz, Matthew and Giménez, Mai and Yeung, Legg and Lin, Hanzhao and Keeling, James and Georgiev, Petko and Mincu, Diana and Wu, Boxi and Haykal, Salem and Saputro, Rachel and Vodrahalli, Kiran and Qin, James and Cankara, Zeynep and Sharma, Abhanshu and Fernando, Nick and Hawkins, Will and Neyshabur, Behnam and Kim, Solomon and Hutter, Adrian and Agrawal, Priyanka and Castro-Ros, Alex and Driessche, George van den and Wang, Tao and Yang, Fan and Chang, Shuo-yiin and Komarek, Paul and McIlroy, Ross and Lučić, Mario and Zhang, Guodong and Farhan, Wael and Sharman, Michael and Natsev, Paul and Michel, Paul and Cheng, Yong and Bansal, Yamini and Qiao, Siyuan and Cao, Kris and Shakeri, Siamak and Butterfield, Christina and Chung, Justin and Rubenstein, Paul Kishan and Agrawal, Shivani and Mensch, Arthur and Soparkar, Kedar and Lenc, Karel and Chung, Timothy and Pope, Aedan and Maggiore, Loren and Kay, Jackie and Jhakra, Priya and Wang, Shibo and Maynez, Joshua and Phuong, Mary and Tobin, Taylor and Tacchetti, Andrea and Trebacz, Maja and Robinson, Kevin and Katariya, Yash and Riedel, Sebastian and Bailey, Paige and Xiao, Kefan and Ghelani, Nimesh and Aroyo, Lora and Slone, Ambrose and Houlsby, Neil and Xiong, Xuehan and Yang, Zhen and Gribovskaya, Elena and Adler, Jonas and Wirth, Mateo and Lee, Lisa and Li, Music and Kagohara, Thais and Pavagadhi, Jay and Bridgers, Sophie and Bortsova, Anna and Ghemawat, Sanjay and Ahmed, Zafarali and Liu, Tianqi and Powell, Richard and Bolina, Vijay and Iinuma, Mariko and Zablotskaia, Polina and Besley, James and Chung, Da-Woon and Dozat, Timothy and Comanescu, Ramona and Si, Xiance and Greer, Jeremy and Su, Guolong and Polacek, Martin and Kaufman, Raphaël Lopez and Tokumine, Simon and Hu, Hexiang and Buchatskaya, Elena and Miao, Yingjie and Elhawaty, Mohamed and Siddhant, Aditya and Tomasev, Nenad and Xing, Jinwei and Greer, Christina and Miller, Helen and Ashraf, Shereen and Roy, Aurko and Zhang, Zizhao and Ma, Ada and Filos, Angelos and Besta, Milos and Blevins, Rory and Klimenko, Ted and Yeh, Chih-Kuan and Changpinyo, Soravit and Mu, Jiaqi and Chang, Oscar and Pajarskas, Mantas and Muir, Carrie and Cohen, Vered and Lan, Charline Le and Haridasan, Krishna and Marathe, Amit and Hansen, Steven and Douglas, Sholto and Samuel, Rajkumar and Wang, Mingqiu and Austin, Sophia and Lan, Chang and Jiang, Jiepu and Chiu, Justin and Lorenzo, Jaime Alonso and Sjösund, Lars Lowe and Cevey, Sébastien and Gleicher, Zach and Avrahami, Thi and Boral, Anudhyan and Srinivasan, Hansa and Selo, Vittorio and May, Rhys and Aisopos, Konstantinos and Hussenot, Léonard and Soares, Livio Baldini and Baumli, Kate and Chang, Michael B. and Recasens, Adrià and Caine, Ben and Pritzel, Alexander and Pavetic, Filip and Pardo, Fabio and Gergely, Anita and Frye, Justin and Ramasesh, Vinay and Horgan, Dan and Badola, Kartikeya and Kassner, Nora and Roy, Subhrajit and Dyer, Ethan and Campos, Víctor and Tomala, Alex and Tang, Yunhao and Badawy, Dalia El and White, Elspeth and Mustafa, Basil and Lang, Oran and Jindal, Abhishek and Vikram, Sharad and Gong, Zhitao and Caelles, Sergi and Hemsley, Ross and Thornton, Gregory and Feng, Fangxiaoyu and Stokowiec, Wojciech and Zheng, Ce and Thacker, Phoebe and Ünlü, Çağlar and Zhang, Zhishuai and Saleh, Mohammad and Svensson, James and Bileschi, Max and Patil, Piyush and Anand, Ankesh and Ring, Roman and Tsihlas, Katerina and Vezer, Arpi and Selvi, Marco and Shevlane, Toby and Rodriguez, Mikel and Kwiatkowski, Tom and Daruki, Samira and Rong, Keran and Dafoe, Allan and FitzGerald, Nicholas and Gu-Lemberg, Keren and Khan, Mina and Hendricks, Lisa Anne and Pellat, Marie and Feinberg, Vladimir and Cobon-Kerr, James and Sainath, Tara and Rauh, Maribeth and Hashemi, Sayed Hadi and Ives, Richard and Hasson, Yana and Li, YaGuang and Noland, Eric and Cao, Yuan and Byrd, Nathan and Hou, Le and Wang, Qingze and Sottiaux, Thibault and Paganini, Michela and Lespiau, Jean-Baptiste and Moufarek, Alexandre and Hassan, Samer and Shivakumar, Kaushik and van Amersfoort, Joost and Mandhane, Amol and Joshi, Pratik and Goyal, Anirudh and Tung, Matthew and Brock, Andrew and Sheahan, Hannah and Misra, Vedant and Li, Cheng and Rakićević, Nemanja and Dehghani, Mostafa and Liu, Fangyu and Mittal, Sid and Oh, Junhyuk and Noury, Seb and Sezener, Eren and Huot, Fantine and Lamm, Matthew and De Cao, Nicola and Chen, Charlie and Elsayed, Gamaleldin and Chi, Ed and Mahdieh, Mahdis and Tenney, Ian and Hua, Nan and Petrychenko, Ivan and Kane, Patrick and Scandinaro, Dylan and Jain, Rishub and Uesato, Jonathan and Datta, Romina and Sadovsky, Adam and Bunyan, Oskar and Rabiej, Dominik and Wu, Shimu and Zhang, John and Vasudevan, Gautam and Leurent, Edouard and Alnahlawi, Mahmoud and Georgescu, Ionut and Wei, Nan and Zheng, Ivy and Chan, Betty and Rabinovitch, Pam G. and Stanczyk, Piotr and Zhang, Ye and Steiner, David and Naskar, Subhajit and Azzam, Michael and Johnson, Matthew and Paszke, Adam and Chiu, Chung-Cheng and Elias, Jaume Sanchez and Mohiuddin, Afroz and Muhammad, Faizan and Miao, Jin and Lee, Andrew and Vieillard, Nino and Potluri, Sahitya and Park, Jane and Davoodi, Elnaz and Zhang, Jiageng and Stanway, Jeff and Garmon, Drew and Karmarkar, Abhijit and Dong, Zhe and Lee, Jong and Kumar, Aviral and Zhou, Luowei and Evens, Jonathan and Isaac, William and Chen, Zhe and Jia, Johnson and Levskaya, Anselm and Zhu, Zhenkai and Gorgolewski, Chris and Grabowski, Peter and Mao, Yu and Magni, Alberto and Yao, Kaisheng and Snaider, Javier and Casagrande, Norman and Suganthan, Paul and Palmer, Evan and Irving, Geoffrey and Loper, Edward and Faruqui, Manaal and Arkatkar, Isha and Chen, Nanxin and Shafran, Izhak and Fink, Michael and Castaño, Alfonso and Giannoumis, Irene and Kim, Wooyeol and Rybiński, Mikołaj and Sreevatsa, Ashwin and Prendki, Jennifer and Soergel, David and Goedeckemeyer, Adrian and Gierke, Willi and Jafari, Mohsen and Gaba, Meenu and Wiesner, Jeremy and Wright, Diana Gage and Wei, Yawen and Vashisht, Harsha and Kulizhskaya, Yana and Hoover, Jay and Le, Maigo and Li, Lu and Iwuanyanwu, Chimezie and Liu, Lu and Ramirez, Kevin and Khorlin, Andrey and Cui, Albert and LIN, Tian and Georgiev, Marin and Wu, Marcus and Aguilar, Ricardo and Pallo, Keith and Chakladar, Abhishek and Repina, Alena and Wu, Xihui and van der Weide, Tom and Ponnapalli, Priya and Kaplan, Caroline and Simsa, Jiri and Li, Shuangfeng and Dousse, Olivier and Yang, Fan and Piper, Jeff and Ie, Nathan and Lui, Minnie and Pasumarthi, Rama and Lintz, Nathan and Vijayakumar, Anitha and Thiet, Lam Nguyen and Andor, Daniel and Valenzuela, Pedro and Paduraru, Cosmin and Peng, Daiyi and Lee, Katherine and Zhang, Shuyuan and Greene, Somer and Nguyen, Duc Dung and Kurylowicz, Paula and Velury, Sarmishta and Krause, Sebastian and Hardin, Cassidy and Dixon, Lucas and Janzer, Lili and Choo, Kiam and Feng, Ziqiang and Zhang, Biao and Singhal, Achintya and Latkar, Tejasi and Zhang, Mingyang and Le, Quoc and Abellan, Elena Allica and Du, Dayou and McKinnon, Dan and Antropova, Natasha and Bolukbasi, Tolga and Keller, Orgad and Reid, David and Finchelstein, Daniel and Raad, Maria Abi and Crocker, Remi and Hawkins, Peter and Dadashi, Robert and Gaffney, Colin and Lall, Sid and Franko, Ken and Filonov, Egor and Bulanova, Anna and Leblond, Rémi and Yadav, Vikas and Chung, Shirley and Askham, Harry and Cobo, Luis C. and Xu, Kelvin and Fischer, Felix and Xu, Jun and Sorokin, Christina and Alberti, Chris and Lin, Chu-Cheng and Evans, Colin and Zhou, Hao and Dimitriev, Alek and Forbes, Hannah and Banarse, Dylan and Tung, Zora and Liu, Jeremiah and Omernick, Mark and Bishop, Colton and Kumar, Chintu and Sterneck, Rachel and Foley, Ryan and Jain, Rohan and Mishra, Swaroop and Xia, Jiawei and Bos, Taylor and Cideron, Geoffrey and Amid, Ehsan and Piccinno, Francesco and Wang, Xingyu and Banzal, Praseem and Gurita, Petru and Noga, Hila and Shah, Premal and Mankowitz, Daniel J. and Polozov, Alex and Kushman, Nate and Krakovna, Victoria and Brown, Sasha and Bateni, MohammadHossein and Duan, Dennis and Firoiu, Vlad and Thotakuri, Meghana and Natan, Tom and Mohananey, Anhad and Geist, Matthieu and Mudgal, Sidharth and Girgin, Sertan and Li, Hui and Ye, Jiayu and Roval, Ofir and Tojo, Reiko and Kwong, Michael and Lee-Thorp, James and Yew, Christopher and Yuan, Quan and Bagri, Sumit and Sinopalnikov, Danila and Ramos, Sabela and Mellor, John and Sharma, Abhishek and Severyn, Aliaksei and Lai, Jonathan and Wu, Kathy and Cheng, Heng-Tze and Miller, David and Sonnerat, Nicolas and Vnukov, Denis and Greig, Rory and Beattie, Jennifer and Caveness, Emily and Bai, Libin and Eisenschlos, Julian and Korchemniy, Alex and Tsai, Tomy and Jasarevic, Mimi and Kong, Weize and Dao, Phuong and Zheng, Zeyu and Liu, Frederick and Yang, Fan and Zhu, Rui and Geller, Mark and Teh, Tian Huey and Sanmiya, Jason and Gladchenko, Evgeny and Trdin, Nejc and Sozanschi, Andrei and Toyama, Daniel and Rosen, Evan and Tavakkol, Sasan and Xue, Linting and Elkind, Chen and Woodman, Oliver and Carpenter, John and Papamakarios, George and Kemp, Rupert and Kafle, Sushant and Grunina, Tanya and Sinha, Rishika and Talbert, Alice and Goyal, Abhimanyu and Wu, Diane and Owusu-Afriyie, Denese and Du, Cosmo and Thornton, Chloe and Pont-Tuset, Jordi and Narayana, Pradyumna and Li, Jing and Fatehi, Sabaer and Wieting, John and Ajmeri, Omar and Uria, Benigno and Zhu, Tao and Ko, Yeongil and Knight, Laura and Héliou, Amélie and Niu, Ning and Gu, Shane and Pang, Chenxi and Tran, Dustin and Li, Yeqing and Levine, Nir and Stolovich, Ariel and Kalb, Norbert and Santamaria-Fernandez, Rebeca and Goenka, Sonam and Yustalim, Wenny and Strudel, Robin and Elqursh, Ali and Lakshminarayanan, Balaji and Deck, Charlie and Upadhyay, Shyam and Lee, Hyo and Dusenberry, Mike and Li, Zonglin and Wang, Xuezhi and Levin, Kyle and Hoffmann, Raphael and Holtmann-Rice, Dan and Bachem, Olivier and Yue, Summer and Arora, Sho and Malmi, Eric and Mirylenka, Daniil and Tan, Qijun and Koh, Christy and Yeganeh, Soheil Hassas and Põder, Siim and Zheng, Steven and Pongetti, Francesco and Tariq, Mukarram and Sun, Yanhua and Ionita, Lucian and Seyedhosseini, Mojtaba and Tafti, Pouya and Kotikalapudi, Ragha and Liu, Zhiyu and Gulati, Anmol and Liu, Jasmine and Ye, Xinyu and Chrzaszcz, Bart and Wang, Lily and Sethi, Nikhil and Li, Tianrun and Brown, Ben and Singh, Shreya and Fan, Wei and Parisi, Aaron and Stanton, Joe and Kuang, Chenkai and Koverkathu, Vinod and Choquette-Choo, Christopher A. and Li, Yunjie and Lu, T. J. and Ittycheriah, Abe and Shroff, Prakash and Sun, Pei and Varadarajan, Mani and Bahargam, Sanaz and Willoughby, Rob and Gaddy, David and Dasgupta, Ishita and Desjardins, Guillaume and Cornero, Marco and Robenek, Brona and Mittal, Bhavishya and Albrecht, Ben and Shenoy, Ashish and Moiseev, Fedor and Jacobsson, Henrik and Ghaffarkhah, Alireza and Rivière, Morgane and Walton, Alanna and Crepy, Clément and Parrish, Alicia and Liu, Yuan and Zhou, Zongwei and Farabet, Clement and Radebaugh, Carey and Srinivasan, Praveen and van der Salm, Claudia and Fidjeland, Andreas and Scellato, Salvatore and Latorre-Chimoto, Eri and Klimczak-Plucińska, Hanna and Bridson, David and de Cesare, Dario and Hudson, Tom and Mendolicchio, Piermaria and Walker, Lexi and Morris, Alex and Penchev, Ivo and Mauger, Matthew and Guseynov, Alexey and Reid, Alison and Odoom, Seth and Loher, Lucia and Cotruta, Victor and Yenugula, Madhavi and Grewe, Dominik and Petrushkina, Anastasia and Duerig, Tom and Sanchez, Antonio and Yadlowsky, Steve and Shen, Amy and Globerson, Amir and Kurzrok, Adam and Webb, Lynette and Dua, Sahil and Li, Dong and Lahoti, Preethi and Bhupatiraju, Surya and Hurt, Dan and Qureshi, Haroon and Agarwal, Ananth and Shani, Tomer and Eyal, Matan and Khare, Anuj and Belle, Shreyas Rammohan and Wang, Lei and Tekur, Chetan and Kale, Mihir Sanjay and Wei, Jinliang and Sang, Ruoxin and Saeta, Brennan and Liechty, Tyler and Sun, Yi and Zhao, Yao and Lee, Stephan and Nayak, Pandu and Fritz, Doug and Vuyyuru, Manish Reddy and Aslanides, John and Vyas, Nidhi and Wicke, Martin and Ma, Xiao and Bilal, Taylan and Eltyshev, Evgenii and Balle, Daniel and Martin, Nina and Cate, Hardie and Manyika, James and Amiri, Keyvan and Kim, Yelin and Xiong, Xi and Kang, Kai and Luisier, Florian and Tripuraneni, Nilesh and Madras, David and Guo, Mandy and Waters, Austin and Wang, Oliver and Ainslie, Joshua and Baldridge, Jason and Zhang, Han and Pruthi, Garima and Bauer, Jakob and Yang, Feng and Mansour, Riham and Gelman, Jason and Xu, Yang and Polovets, George and Liu, Ji and Cai, Honglong and Chen, Warren and Sheng, XiangHai and Xue, Emily and Ozair, Sherjil and Yu, Adams and Angermueller, Christof and Li, Xiaowei and Wang, Weiren and Wiesinger, Julia and Koukoumidis, Emmanouil and Tian, Yuan and Iyer, Anand and Gurumurthy, Madhu and Goldenson, Mark and Shah, Parashar and Blake, M. K. and Yu, Hongkun and Urbanowicz, Anthony and Palomaki, Jennimaria and Fernando, Chrisantha and Brooks, Kevin and Durden, Ken and Mehta, Harsh and Momchev, Nikola and Rahimtoroghi, Elahe and Georgaki, Maria and Raul, Amit and Ruder, Sebastian and Redshaw, Morgan and Lee, Jinhyuk and Jalan, Komal and Li, Dinghua and Perng, Ginger and Hechtman, Blake and Schuh, Parker and Nasr, Milad and Chen, Mia and Milan, Kieran and Mikulik, Vladimir and Strohman, Trevor and Franco, Juliana and Green, Tim and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeffrey and Vinyals, Oriol},
	month = dec,
	year = {2023},
	note = {arXiv:2312.11805 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/7BR7C5H4/Gemini Team et al. - 2023 - Gemini A Family of Highly Capable Multimodal Mode.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/V2HEBMCE/2312.html:text/html},
}

@misc{smith_walk_2022,
	title = {A {Walk} in the {Park}: {Learning} to {Walk} in 20 {Minutes} {With} {Model}-{Free} {Reinforcement} {Learning}},
	shorttitle = {A {Walk} in the {Park}},
	url = {http://arxiv.org/abs/2208.07860},
	doi = {10.48550/arXiv.2208.07860},
	abstract = {Deep reinforcement learning is a promising approach to learning policies in uncontrolled environments that do not require domain knowledge. Unfortunately, due to sample inefficiency, deep RL applications have primarily focused on simulated environments. In this work, we demonstrate that the recent advancements in machine learning algorithms and libraries combined with a carefully tuned robot controller lead to learning quadruped locomotion in only 20 minutes in the real world. We evaluate our approach on several indoor and outdoor terrains which are known to be challenging for classical model-based controllers. We observe the robot to be able to learn walking gait consistently on all of these terrains. Finally, we evaluate our design decisions in a simulated environment.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
	month = aug,
	year = {2022},
	note = {arXiv:2208.07860 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/WYZ7VLAX/Smith et al. - 2022 - A Walk in the Park Learning to Walk in 20 Minutes.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/PU7ZEHFQ/2208.html:text/html},
}

@misc{driess_palm-e_2023,
	title = {{PaLM}-{E}: {An} {Embodied} {Multimodal} {Language} {Model}},
	shorttitle = {{PaLM}-{E}},
	url = {http://arxiv.org/abs/2303.03378},
	doi = {10.48550/arXiv.2303.03378},
	abstract = {Large language models excel at a wide range of complex tasks. However, enabling general inference in the real world, e.g., for robotics problems, raises the challenge of grounding. We propose embodied language models to directly incorporate real-world continuous sensor modalities into language models and thereby establish the link between words and percepts. Input to our embodied language model are multi-modal sentences that interleave visual, continuous state estimation, and textual input encodings. We train these encodings end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks including sequential robotic manipulation planning, visual question answering, and captioning. Our evaluations show that PaLM-E, a single large embodied multimodal model, can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains. Our largest model, PaLM-E-562B with 562B parameters, in addition to being trained on robotics tasks, is a visual-language generalist with state-of-the-art performance on OK-VQA, and retains generalist language capabilities with increasing scale.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
	month = mar,
	year = {2023},
	note = {arXiv:2303.03378 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/ABW2FCDG/Driess et al. - 2023 - PaLM-E An Embodied Multimodal Language Model.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/7NBBFN5W/2303.html:text/html},
}

@misc{noauthor_rt-2_nodate,
	title = {{RT}-2: {New} model translates vision and language into action - {Google} {DeepMind}},
	url = {https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/},
	urldate = {2024-02-26},
}

@misc{fu_mobile_2024,
	title = {Mobile {ALOHA}: {Learning} {Bimanual} {Mobile} {Manipulation} with {Low}-{Cost} {Whole}-{Body} {Teleoperation}},
	shorttitle = {Mobile {ALOHA}},
	url = {http://arxiv.org/abs/2401.02117},
	doi = {10.48550/arXiv.2401.02117},
	abstract = {Imitation learning from human demonstrations has shown impressive performance in robotics. However, most results focus on table-top manipulation, lacking the mobility and dexterity necessary for generally useful tasks. In this work, we develop a system for imitating mobile manipulation tasks that are bimanual and require whole-body control. We first present Mobile ALOHA, a low-cost and whole-body teleoperation system for data collection. It augments the ALOHA system with a mobile base, and a whole-body teleoperation interface. Using data collected with Mobile ALOHA, we then perform supervised behavior cloning and find that co-training with existing static ALOHA datasets boosts performance on mobile manipulation tasks. With 50 demonstrations for each task, co-training can increase success rates by up to 90\%, allowing Mobile ALOHA to autonomously complete complex mobile manipulation tasks such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet. Project website: https://mobile-aloha.github.io},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Fu, Zipeng and Zhao, Tony Z. and Finn, Chelsea},
	month = jan,
	year = {2024},
	note = {arXiv:2401.02117 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/LSBDK4WQ/Fu et al. - 2024 - Mobile ALOHA Learning Bimanual Mobile Manipulatio.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/MSFV8PXA/2401.html:text/html},
}

@misc{noauthor_deep_2024,
	title = {Deep {Blue} (chess computer)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Deep_Blue_(chess_computer)&oldid=1210343921},
	abstract = {Deep Blue was a chess-playing expert system run on a unique purpose-built IBM supercomputer. It was the first computer to win a game, and the first to win a match, against a reigning world champion under regular time controls. Development began in 1985 at Carnegie Mellon University under the name ChipTest. It then moved to IBM, where it was first renamed Deep Thought, then again in 1989 to Deep Blue. It first played world champion Garry Kasparov in a six-game match in 1996, where it lost four games to two. It was upgraded in 1997 and in a six-game re-match, it defeated Kasparov by winning two games and drawing three. Deep Blue's victory is considered a milestone in the history of artificial intelligence and has been the subject of several books and films.},
	language = {en},
	urldate = {2024-02-26},
	journal = {Wikipedia},
	month = feb,
	year = {2024},
	note = {Page Version ID: 1210343921},
}

@misc{noauthor_scrabble_nodate,
	title = {Scrabble {Showdown}: {Quackle} vs. {David} {Boys} - {Top} 10 {Man}-vs.-{Machine} {Moments} - {TIME}},
	url = {https://content.time.com/time/specials/packages/article/0,28804,2049187_2049195_2049083,00.html},
	urldate = {2024-02-26},
	file = {Scrabble Showdown\: Quackle vs. David Boys - Top 10 Man-vs.-Machine Moments - TIME:/home/jeanne/Zotero/storage/4FNM4NBG/0,28804,2049187_2049195_2049083,00.html:text/html},
}

@article{webley_top_2011,
	title = {Top 10 {Man}-vs.-{Machine} {Moments} - {TIME}},
	issn = {0040-781X},
	url = {https://content.time.com/time/specials/packages/article/0,28804,2049187_2049195_2049083,00.html},
	abstract = {Using words like qadi (a Muslim judge), anuria (the nonpassage of urine) and alif (a type of spinal fusion), a software program called Quackle beat David Boys 482-465 in the final round of the 2006...},
	language = {en-US},
	urldate = {2024-02-26},
	journal = {Time},
	author = {Webley, Kayla},
	month = feb,
	year = {2011},
	keywords = {computers, scrabble, top 10 man vs. machine moments},
}

@misc{noauthor_alphago_2020,
	title = {{AlphaGo}},
	url = {https://deepmind.google/technologies/alphago/},
	abstract = {Novel AI system mastered the ancient game of Go, defeated a Go world champion, and inspired a new era of AI.},
	language = {en},
	urldate = {2024-02-26},
	journal = {Google DeepMind},
	month = dec,
	year = {2020},
}

@misc{noauthor_alphazero_nodate,
	title = {{AlphaZero}: {Shedding} new light on chess, shogi, and {Go} - {Google} {DeepMind}},
	url = {https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/},
	urldate = {2024-02-26},
}

@misc{mnih_playing_2013,
	title = {Playing {Atari} with {Deep} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1312.5602},
	doi = {10.48550/arXiv.1312.5602},
	abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
	month = dec,
	year = {2013},
	note = {arXiv:1312.5602 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/BHAMB86A/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/5L6NMHY4/1312.html:text/html},
}

@misc{noauthor_openai_nodate,
	title = {{OpenAI} {Five} defeats {Dota} 2 world champions},
	url = {https://openai.com/research/openai-five-defeats-dota-2-world-champions},
	urldate = {2024-02-26},
}

@misc{noauthor_alphastar_nodate,
	title = {{AlphaStar}: {Mastering} the real-time strategy game {StarCraft} {II} - {Google} {DeepMind}},
	url = {https://deepmind.google/discover/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii/},
	urldate = {2024-02-26},
}

@misc{noauthor_muzero_2020,
	title = {{MuZero}: {Mastering} {Go}, chess, shogi and {Atari} without rules},
	shorttitle = {{MuZero}},
	url = {https://deepmind.google/discover/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules/},
	abstract = {In 2016, we introduced AlphaGo, the first artificial intelligence (AI) program to defeat humans at the ancient game of Go. Two years later, its successor - AlphaZero - learned from scratch to master Go, chess and shogi. Now, in a paper in the journal Nature, we describe MuZero, a significant step forward in the pursuit of general-purpose algorithms. MuZero masters Go, chess, shogi and Atari without needing to be told the rules, thanks to its ability to plan winning strategies in unknown environments.},
	language = {en},
	urldate = {2024-02-26},
	journal = {Google DeepMind},
	month = dec,
	year = {2020},
}

@misc{bakhtin_mastering_2022,
	title = {Mastering the {Game} of {No}-{Press} {Diplomacy} via {Human}-{Regularized} {Reinforcement} {Learning} and {Planning}},
	url = {http://arxiv.org/abs/2210.05492},
	doi = {10.48550/arXiv.2210.05492},
	abstract = {No-press Diplomacy is a complex strategy game involving both cooperation and competition that has served as a benchmark for multi-agent AI research. While self-play reinforcement learning has resulted in numerous successes in purely adversarial games like chess, Go, and poker, self-play alone is insufficient for achieving optimal performance in domains involving cooperation with humans. We address this shortcoming by first introducing a planning algorithm we call DiL-piKL that regularizes a reward-maximizing policy toward a human imitation-learned policy. We prove that this is a no-regret learning algorithm under a modified utility function. We then show that DiL-piKL can be extended into a self-play reinforcement learning algorithm we call RL-DiL-piKL that provides a model of human play while simultaneously training an agent that responds well to this human model. We used RL-DiL-piKL to train an agent we name Diplodocus. In a 200-game no-press Diplomacy tournament involving 62 human participants spanning skill levels from beginner to expert, two Diplodocus agents both achieved a higher average score than all other participants who played more than two games, and ranked first and third according to an Elo ratings model.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Bakhtin, Anton and Wu, David J. and Lerer, Adam and Gray, Jonathan and Jacob, Athul Paul and Farina, Gabriele and Miller, Alexander H. and Brown, Noam},
	month = oct,
	year = {2022},
	note = {arXiv:2210.05492 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/4WHFXGWG/Bakhtin et al. - 2022 - Mastering the Game of No-Press Diplomacy via Human.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/JLNI75M5/2210.html:text/html},
}

@misc{noauthor_expert_nodate,
	title = {Expert {Diplomacy} {Player} vs {CICERO} {AI} - {YouTube}},
	url = {https://www.youtube.com/watch?v=u5192bvUS7k&t=2216s},
	urldate = {2024-02-26},
}

@misc{radford_robust_2022,
	title = {Robust {Speech} {Recognition} via {Large}-{Scale} {Weak} {Supervision}},
	url = {http://arxiv.org/abs/2212.04356},
	doi = {10.48550/arXiv.2212.04356},
	abstract = {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without the need for any fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04356 [cs, eess]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/4ZL2B6Y7/Radford et al. - 2022 - Robust Speech Recognition via Large-Scale Weak Sup.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/MD8VH9P5/2212.html:text/html},
}

@misc{noauthor_230516291_nodate,
	title = {[2305.16291] {Voyager}: {An} {Open}-{Ended} {Embodied} {Agent} with {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2305.16291},
	urldate = {2024-02-26},
	file = {[2305.16291] Voyager\: An Open-Ended Embodied Agent with Large Language Models:/home/jeanne/Zotero/storage/MIEKL9E4/2305.html:text/html},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/E4UDZ4ZN/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/HLXK5P8E/2108.html:text/html},
}

@misc{noauthor_linkedin_nodate,
	title = {{LinkedIn}},
	url = {https://www.linkedin.com/pulse/altman-multimodality-important-david-cronshaw-5fz0c},
	urldate = {2024-02-26},
}

@misc{legg_formal_2006,
	title = {A {Formal} {Measure} of {Machine} {Intelligence}},
	url = {http://arxiv.org/abs/cs/0605024},
	doi = {10.48550/arXiv.cs/0605024},
	abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this measure formally captures the concept of machine intelligence in the broadest reasonable sense.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Legg, Shane and Hutter, Marcus},
	month = may,
	year = {2006},
	note = {arXiv:cs/0605024},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/M5HERNRU/Legg et Hutter - 2006 - A Formal Measure of Machine Intelligence.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/VHNB53XM/0605024.html:text/html},
}

@misc{chollet_measure_2019,
	title = {On the {Measure} of {Intelligence}},
	url = {http://arxiv.org/abs/1911.01547},
	doi = {10.48550/arXiv.1911.01547},
	abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Chollet, François},
	month = nov,
	year = {2019},
	note = {arXiv:1911.01547 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/DX7QXDDL/Chollet - 2019 - On the Measure of Intelligence.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/44UF7YTF/1911.html:text/html},
}

@misc{noauthor_when_nodate,
	title = {When discussing {AI} risks, talk about capabilities, not intelligence — {AI} {Alignment} {Forum}},
	url = {https://www.alignmentforum.org/posts/JtuTQgp9Wnd6R6F5s/when-discussing-ai-risks-talk-about-capabilities-not},
	urldate = {2024-02-26},
}

@misc{legg_universal_2007,
	title = {Universal {Intelligence}: {A} {Definition} of {Machine} {Intelligence}},
	shorttitle = {Universal {Intelligence}},
	url = {http://arxiv.org/abs/0712.3329},
	doi = {10.48550/arXiv.0712.3329},
	abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Legg, Shane and Hutter, Marcus},
	month = dec,
	year = {2007},
	note = {arXiv:0712.3329 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/3PAFM8M2/Legg et Hutter - 2007 - Universal Intelligence A Definition of Machine In.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/IWZPKBVY/0712.html:text/html},
}

@misc{noauthor_updates_nodate,
	title = {Updates to the {OECD}’s definition of an {AI} system explained - {OECD}.{AI}},
	url = {https://oecd.ai/en/wonk/ai-system-definition-update},
	urldate = {2024-02-26},
}

@misc{noauthor_what_nodate,
	title = {What is {Artificial} {Intelligence} ({AI})? {\textbar} {IBM}},
	url = {https://www.ibm.com/topics/artificial-intelligence},
	urldate = {2024-02-26},
}

@misc{noauthor_what_nodate-1,
	title = {What is {AGI}? - {Machine} {Intelligence} {Research} {Institute}},
	url = {https://intelligence.org/2013/08/11/what-is-agi/},
	urldate = {2024-02-26},
}

@misc{muehlhauser_what_2013,
	title = {What is {Intelligence}?},
	url = {https://intelligence.org/2013/06/19/what-is-intelligence-2/},
	abstract = {When asked their opinions about “human-level artificial intelligence” — aka “artificial general intelligence” (AGI)1 — many experts understandably reply that these terms haven’t yet been precisely defined, and it’s hard to talk about something that hasn’t been defined.2 In this post, I want to briefly outline an imprecise but useful “working definition” for intelligence we... Read more »},
	language = {en-US},
	urldate = {2024-02-26},
	journal = {Machine Intelligence Research Institute},
	author = {Muehlhauser, Luke},
	month = jun,
	year = {2013},
}

@misc{noauthor_background_nodate,
	title = {Some {Background} on {Our} {Views} {Regarding} {Advanced} {Artificial} {Intelligence}},
	url = {https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/},
	abstract = {We’re planning to make potential risks from advanced artificial intelligence a major priority in 2016. A future post will discuss why; this post gives some background. Summary: I first give our definition of “transformative artificial intelligence,” our term for a type of potential advanced artificial intelligence we find particularly relevant for our purposes. Roughly and […]},
	language = {en-us},
	urldate = {2024-02-26},
	journal = {Open Philanthropy},
}

@misc{noauthor_x_nodate,
	title = {X},
	url = {https://twitter.com/ylecun/status/1639696127132835840},
	urldate = {2024-02-26},
	file = {X:/home/jeanne/Zotero/storage/6R423XX5/1639696127132835840.html:text/html},
}

@misc{greenblatt_ai_2024,
	title = {{AI} {Control}: {Improving} {Safety} {Despite} {Intentional} {Subversion}},
	shorttitle = {{AI} {Control}},
	url = {http://arxiv.org/abs/2312.06942},
	doi = {10.48550/arXiv.2312.06942},
	abstract = {As large language models (LLMs) become more powerful and are deployed more autonomously, it will be increasingly important to prevent them from causing harmful outcomes. Researchers have investigated a variety of safety techniques for this purpose, e.g. using models to review the outputs of other models, or red-teaming techniques to surface subtle failure modes. However, researchers have not evaluated whether such techniques still ensure safety if the model is itself intentionally trying to subvert them. In this paper, we develop and evaluate pipelines of safety techniques ("protocols") that are robust to intentional subversion. We investigate a scenario in which we want to solve a sequence of programming problems, using access to a powerful but untrusted model (in our case, GPT-4), access to a less powerful trusted model (in our case, GPT-3.5), and limited access to high-quality trusted labor. We investigate protocols that aim to never submit solutions containing backdoors, which we operationalize here as logical errors that are not caught by test cases. We investigate a range of protocols and test each against strategies that the untrusted model could use to subvert them. One protocol is what we call trusted editing. This protocol first asks GPT-4 to write code, and then asks GPT-3.5 to rate the suspiciousness of that code. If the code is below some suspiciousness threshold, it is submitted. Otherwise, GPT-3.5 edits the solution to remove parts that seem suspicious and then submits the edited code. Another protocol is untrusted monitoring. This protocol asks GPT-4 to write code, and then asks another instance of GPT-4 whether the code is backdoored, using various techniques to prevent the GPT-4 instances from colluding. These protocols improve substantially on simple baselines.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Greenblatt, Ryan and Shlegeris, Buck and Sachan, Kshitij and Roger, Fabien},
	month = jan,
	year = {2024},
	note = {arXiv:2312.06942 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/PY2GW9RB/Greenblatt et al. - 2024 - AI Control Improving Safety Despite Intentional S.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/N4ADF8HA/2312.html:text/html},
}

@misc{noauthor_moores_2024,
	title = {Moore's law},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Moore%27s_law&oldid=1210210867},
	abstract = {Moore's law is the observation that the number of transistors in an integrated circuit (IC) doubles about every two years. Moore's law is an observation and projection of a historical trend. Rather than a law of physics, it is an empirical relationship linked to gains from experience in production.
The observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and Intel (and former CEO of the latter), who in 1965 posited a doubling every year in the number of components per integrated circuit, and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 41\%. While Moore did not use empirical evidence in forecasting that the historical trend would continue, his prediction has held since 1975 and has since become known as a "law".
Moore's prediction has been used in the semiconductor industry to guide long-term planning and to set targets for research and development, thus functioning to some extent as a self-fulfilling prophecy. Advancements in digital electronics, such as the reduction in quality-adjusted microprocessor prices, the increase in memory capacity (RAM and flash), the improvement of sensors, and even the number and size of pixels in digital cameras, are strongly linked to Moore's law. These ongoing changes in digital electronics have been a driving force of technological and social change, productivity, and economic growth.
Industry experts have not reached a consensus on exactly when Moore's law will cease to apply. Microprocessor architects report that semiconductor advancement has slowed industry-wide since around 2010, slightly below the pace predicted by Moore's law. In September 2022, Nvidia CEO Jensen Huang considered Moore's law dead, while Intel CEO Pat Gelsinger was of the opposite view.},
	language = {en},
	urldate = {2024-02-26},
	journal = {Wikipedia},
	month = feb,
	year = {2024},
	note = {Page Version ID: 1210210867},
}

@misc{noauthor_bitter_nodate,
	title = {The {Bitter} {Lesson}},
	url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
	urldate = {2024-02-26},
	file = {The Bitter Lesson:/home/jeanne/Zotero/storage/2BMIMBSZ/BitterLesson.html:text/html},
}

@misc{noauthor_parti_nodate,
	title = {Parti: {Pathways} {Autoregressive} {Text}-to-{Image} {Model}},
	url = {https://sites.research.google/parti/},
	urldate = {2024-02-26},
}

@misc{noauthor_what_nodate-2,
	title = {What {DALL}-{E} 2 can and cannot do — {LessWrong}},
	url = {https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do},
	urldate = {2024-02-26},
}

@misc{noauthor_new_nodate,
	title = {New {Scaling} {Laws} for {Large} {Language} {Models} — {LessWrong}},
	url = {https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models},
	urldate = {2024-02-26},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/JGHIAL3N/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/XPH2H37Q/2001.html:text/html},
}

@misc{noauthor_200108361_nodate,
	title = {[2001.08361'] {Article} identifier not recognized},
	url = {https://arxiv.org/abs/2001.08361%27},
	urldate = {2024-02-26},
	file = {[2001.08361'] Article identifier not recognized:/home/jeanne/Zotero/storage/FBCZ6WW3/2001.html:text/html},
}

@misc{noauthor_scaling_nodate,
	title = {The {Scaling} {Hypothesis} · {Gwern}.net},
	url = {https://gwern.net/scaling-hypothesis#scaling-hypothesis},
	urldate = {2024-02-26},
	file = {The Scaling Hypothesis · Gwern.net:/home/jeanne/Zotero/storage/SCMBI55V/scaling-hypothesis.html:text/html},
}

@article{lecun_path_nodate,
	title = {A {Path} {Towards} {Autonomous} {Machine} {Intelligence} {Version} 0.9.2, 2022-06-27},
	abstract = {How could machines learn as eﬃciently as humans and animals? How could machines learn to reason and plan? How could machines learn representations of percepts and action plans at multiple levels of abstraction, enabling them to reason, predict, and plan at multiple time horizons? This position paper proposes an architecture and training paradigms with which to construct autonomous intelligent agents. It combines concepts such as conﬁgurable predictive world model, behavior driven through intrinsic motivation, and hierarchical joint embedding architectures trained with self-supervised learning.},
	language = {en},
	author = {LeCun, Yann},
	file = {LeCun - A Path Towards Autonomous Machine Intelligence Ver.pdf:/home/jeanne/Zotero/storage/ZSR7Q55A/LeCun - A Path Towards Autonomous Machine Intelligence Ver.pdf:application/pdf},
}

@misc{noauthor_httpsopenreviewnetpdfidbz5a1r-kvsf_nodate,
	title = {https://openreview.net/pdf?id={BZ5a1r}-{kVsf}},
	url = {https://openreview.net/pdf?id=BZ5a1r-kVsf},
	urldate = {2024-02-26},
}

@misc{sutton_alberta_2023,
	title = {The {Alberta} {Plan} for {AI} {Research}},
	url = {http://arxiv.org/abs/2208.11173},
	doi = {10.48550/arXiv.2208.11173},
	abstract = {Herein we describe our approach to artificial intelligence research, which we call the Alberta Plan. The Alberta Plan is pursued within our research groups in Alberta and by others who are like minded throughout the world. We welcome all who would join us in this pursuit.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Sutton, Richard S. and Bowling, Michael and Pilarski, Patrick M.},
	month = mar,
	year = {2023},
	note = {arXiv:2208.11173 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/LYLDITH2/Sutton et al. - 2023 - The Alberta Plan for AI Research.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/3ZPATPXN/2208.html:text/html},
}

@misc{noauthor_planning_nodate,
	title = {Planning for {AGI} and beyond},
	url = {https://openai.com/blog/planning-for-agi-and-beyond},
	abstract = {Our mission is to ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity.},
	language = {en-US},
	urldate = {2024-02-26},
}

@misc{noauthor_introducing_nodate,
	title = {Introducing {Superalignment}},
	url = {https://openai.com/blog/introducing-superalignment},
	abstract = {We need scientific and technical breakthroughs to steer and control AI systems much smarter than us. To solve this problem within four years, we’re starting a new team, co-led by Ilya Sutskever and Jan Leike, and dedicating 20\% of the compute we’ve secured to date to this effort. We’re looking for excellent ML researchers and engineers to join us.},
	language = {en-US},
	urldate = {2024-02-26},
}

@misc{dwarkesh_patel_dario_2023,
	title = {Dario {Amodei} ({Anthropic} {CEO}) - \$10 {Billion} {Models}, {OpenAI}, {Scaling}, \& {Alignment}},
	url = {https://www.youtube.com/watch?v=Nlkk3glap_U},
	abstract = {Here is my conversation with Dario Amodei, CEO of Anthropic.

Dario is hilarious and has fascinating takes on what these models are doing, why they scale so well, and what it will take to align them.

Transcript: https://www.dwarkeshpatel.com/dario-a...
Apple Podcasts: https://apple.co/3rZOzPA
Spotify: https://spoti.fi/3QwMXXU

Follow me on Twitter:   / dwarkesh\_sp  

---
I’m running an experiment on this episode.

I’m not doing an ad. 

Instead, I’m just going to ask you to pay for whatever value you feel you personally got out of this conversation.

Pay here: https://bit.ly/3ONINtp
---

(00:00:00) - Introduction
(00:01:00) - Scaling
(00:15:46) - Language
(00:22:58) - Economic Usefulness
(00:38:05) - Bioterrorism
(00:43:35) - Cybersecurity
(00:47:19) - Alignment \& mechanistic interpretability
(00:57:43) - Does alignment research require scale?
(01:05:30) - Misuse vs misalignment
(01:09:06) - What if AI goes well?
(01:11:05) - China
(01:15:11) - How to think about alignment
(01:31:31) - Is modern security good enough?
(01:36:09) - Inefficiencies in training
(01:45:53) - Anthropic’s Long Term Benefit Trust
(01:51:18) - Is Claude conscious?
(01:56:14) - Keeping a low profile},
	urldate = {2024-02-26},
	author = {{Dwarkesh Patel}},
	month = aug,
	year = {2023},
}

@article{zac_kenton_clarifying_nodate,
	title = {Clarifying {AI} {X}-risk},
	url = {https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk},
	abstract = {TL;DR: We give a threat model literature review, propose a categorization and describe a consensus threat model from some of DeepMind's AGI safety te…},
	language = {en},
	urldate = {2024-02-26},
	author = {zac\_kenton and Shah, Rohin and Lindner, David and Varma, Vikrant and Vika and Phuong, Mary and Kumar, Ramana and Catt, Elliot},
}

@article{andrea_miotti_agi_nodate,
	title = {{AGI} in sight: our look at the game board},
	shorttitle = {{AGI} in sight},
	url = {https://www.lesswrong.com/posts/PE22QJSww8mpwh7bt/agi-in-sight-our-look-at-the-game-board},
	abstract = {From our point of view, we are now in the end-game for AGI, and we (humans) are losing. When we share this with other people, they reliably get surpr…},
	language = {en},
	urldate = {2024-02-26},
	author = {Andrea\_Miotti and Alfour, Gabriel},
}

@misc{noauthor_forecasting_nodate,
	title = {Forecasting: {Lecture} {Notes} - 5  {Zeroth} and {First} {Order} {Forecasting}},
	url = {https://forecasting.quarto.pub/book/zeroth-first.html?ref=bounded-regret.ghost.io#first-order-approximation},
	urldate = {2024-02-26},
}

@misc{noauthor_what_2023,
	title = {What will {GPT}-2030 look like?},
	url = {https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/},
	abstract = {GPT-4 surprised many people with its abilities at coding, creative brainstorming, letter-writing, and other skills. How can we be less surprised by developments in machine learning? In this post, I’ll forecast the properties of large pretrained ML systems in 2030.},
	language = {en},
	urldate = {2024-02-26},
	journal = {Bounded Regret},
	month = jun,
	year = {2023},
}

@misc{noauthor_biological_nodate,
	title = {“{Biological} anchors” is about bounding, not pinpointing, {AI} timelines — {EA} {Forum}},
	url = {https://forum.effectivealtruism.org/posts/ajBYeiggAzu6Cgb3o/biological-anchors-is-about-bounding-not-pinpointing-ai},
	urldate = {2024-02-26},
}

@misc{ho_grokking_2022,
	title = {Grokking “{Forecasting} {TAI} {With} {Biological} {Anchors}”},
	url = {https://epochai.org/blog/grokking-bioanchors},
	abstract = {I give a visual explanation of Ajeya Cotra’s draft report, Forecasting TAI with biological anchors, summarising the key assumptions, intuitions, and conclusions.},
	language = {en},
	urldate = {2024-02-26},
	journal = {Epoch},
	author = {Ho, Anson},
	month = jun,
	year = {2022},
}

@misc{noauthor_simple_nodate,
	title = {Simple evolution analogy},
	url = {https://docs.google.com/document/d/1HUtUBpRbNnnWBxiO2bz3LumEsQcaZioAPZDNcsWPnos/edit?usp=embed_facebook},
	language = {fr},
	urldate = {2024-02-26},
	journal = {Google Docs},
	file = {Snapshot:/home/jeanne/Zotero/storage/2R7XIVVN/edit.html:text/html},
}

@article{cotra_draft_nodate,
	title = {Draft report on {AI} timelines},
	url = {https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines},
	abstract = {Hi all, I've been working on some AI forecasting research and have prepared a draft report on timelines to transformative AI. I would love feedback f…},
	language = {en},
	urldate = {2024-02-26},
	author = {Cotra, Ajeya},
}

@misc{noauthor_ai_nodate,
	title = {{AI} and compute},
	url = {https://openai.com/research/ai-and-compute},
	abstract = {We’re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period)[{\textasciicircum}footnote-correction]. Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities.},
	language = {en-US},
	urldate = {2024-02-26},
}

@misc{noauthor_two-year_nodate,
	title = {Two-year update on my personal {AI} timelines — {AI} {Alignment} {Forum}},
	url = {https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines},
	urldate = {2024-02-26},
}

@misc{noauthor_biology-inspired_nodate,
	title = {Biology-{Inspired} {AGI} {Timelines}: {The} {Trick} {That} {Never} {Works} - {Machine} {Intelligence} {Research} {Institute}},
	url = {https://intelligence.org/2021/12/03/biology-inspired-agi-timelines-the-trick-that-never-works/},
	urldate = {2024-02-26},
}

@misc{noauthor_reply_nodate,
	title = {Reply to {Eliezer} on {Biological} {Anchors} — {AI} {Alignment} {Forum}},
	url = {https://www.alignmentforum.org/posts/nNqXfnjiezYukiMJi/reply-to-eliezer-on-biological-anchors},
	urldate = {2024-02-26},
}

@article{martin_takeoff_nodate,
	title = {Takeoff {Speeds} and {Discontinuities}},
	url = {https://www.alignmentforum.org/posts/pGXR2ynhe5bBCCNqn/takeoff-speeds-and-discontinuities},
	abstract = {This post is part 4 in our sequence on Modeling Transformative AI Risk. We are building a model to understand debates around existential risks from a…},
	language = {en},
	urldate = {2024-02-26},
	author = {Martin, Sammy and Daniel\_Eth},
}

@misc{noauthor_homogeneity_nodate,
	title = {Homogeneity vs. heterogeneity in {AI} takeoff scenarios — {LessWrong}},
	url = {https://www.lesswrong.com/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios},
	urldate = {2024-02-26},
}

@inproceedings{yudkowsky_intelligence_2013,
	title = {Intelligence {Explosion} {Microeconomics}},
	url = {https://www.semanticscholar.org/paper/Intelligence-Explosion-Microeconomics-Yudkowsky/74cda7d4006b26bd623f39c8ac16a8151d56a0e5},
	abstract = {I. J. Good’s thesis of the “intelligence explosion” states that a sufficiently advanced machine intelligence could build a smarter version of itself, which could in turn build an even smarter version, and that this process could continue to the point of vastly exceeding human intelligence. As Sandberg (2010) correctly notes, there have been several attempts to lay down return on investment formulas intended to represent sharp speedups in economic or technological growth, but very little attempt has been made to deal formally with Good’s intelligence explosion thesis as such. I identify the key issue as returns on cognitive reinvestment—the ability to invest more computing power, faster computers, or improved cognitive algorithms to yield cognitive labor which produces larger brains, faster brains, or better mind designs. There are many phenomena in the world which have been argued to be evidentially relevant to this question, from the observed course of hominid evolution, to Moore’s Law, to the competenceovertimeofmachinechess-playingsystems, andmanymore. Igointosome depth on some debates which then arise on how to interpret such evidence. I propose that the next step in analyzing positions on the intelligence explosion would be to formalize return on investment curves, so that each stance can formally state which possible microfoundations they hold to be falsified by historical observations. More generally,},
	urldate = {2024-02-26},
	author = {Yudkowsky, Eliezer},
	year = {2013},
	file = {Full Text PDF:/home/jeanne/Zotero/storage/TD3SH5SP/Yudkowsky - 2013 - Intelligence Explosion Microeconomics.pdf:application/pdf},
}

@incollection{muehlhauser_intelligence_2012,
	address = {Berlin, Heidelberg},
	series = {The {Frontiers} {Collection}},
	title = {Intelligence {Explosion}: {Evidence} and {Import}},
	isbn = {978-3-642-32560-1},
	shorttitle = {Intelligence {Explosion}},
	url = {https://doi.org/10.1007/978-3-642-32560-1_2},
	abstract = {In this chapter we review the evidence for and against three claims: that (1) there is a substantial chance we will create human-level AI before 2100, that (2) if human-level AI is created, there is a good chance vastly superhuman AI will follow via an “intelligence explosion,” and that (3) an uncontrolled intelligence explosion could destroy everything we value, but a controlled intelligence explosion would benefit humanity enormously if we can achieve it. We conclude with recommendations for increasing the odds of a controlled intelligence explosion relative to an uncontrolled intelligence explosion.},
	language = {en},
	urldate = {2024-02-26},
	booktitle = {Singularity {Hypotheses}: {A} {Scientific} and {Philosophical} {Assessment}},
	publisher = {Springer},
	author = {Muehlhauser, Luke and Salamon, Anna},
	editor = {Eden, Amnon H. and Moor, James H. and Søraker, Johnny H. and Steinhart, Eric},
	year = {2012},
	doi = {10.1007/978-3-642-32560-1_2},
	keywords = {Artificial Intelligence, Intelligence Software, Machine Intelligence, Optimization Power, Transcranial Magnetic Stimulation},
	pages = {15--42},
	file = {Full Text PDF:/home/jeanne/Zotero/storage/ULQUDCLZ/Muehlhauser et Salamon - 2012 - Intelligence Explosion Evidence and Import.pdf:application/pdf},
}

@misc{noauthor_what_nodate-3,
	title = {What a {Compute}-{Centric} {Framework} {Says} {About} {Takeoff} {Speeds}},
	url = {https://www.openphilanthropy.org/research/what-a-compute-centric-framework-says-about-takeoff-speeds/},
	abstract = {This is Part 0 of a four-part report — see links to Part 1. Part 2. Part 3, and a folder with more materials. Abstract In the next few decades we may develop AI that can automate {\textasciitilde}all cognitive tasks and dramatically transform the world. By contrast, today the capabilities and impact of AI are much […]},
	language = {en-us},
	urldate = {2024-02-26},
	journal = {Open Philanthropy},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	urldate = {2024-02-26},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jeanne/Zotero/storage/M9ACPA6G/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/jeanne/Zotero/storage/SCPE8JII/2203.html:text/html},
}

@misc{noauthor_forecasting_nodate-1,
	title = {Forecasting transformative {AI}: the "biological anchors" method in a nutshell — {EA} {Forum}},
	url = {https://forum.effectivealtruism.org/posts/vCaEnTbZ5KbypaGsm/forecasting-transformative-ai-the-biological-anchors-method},
	urldate = {2024-02-26},
}
