
@misc{noauthor_computers_nodate,
	title = {Computers ace IQ tests but still make dumb mistakes. Can different tests help?},
	url = {https://www.science.org/content/article/computers-ace-iq-tests-still-make-dumb-mistakes-can-different-tests-help},
	abstract = {AI researchers are creating novel “benchmarks” to help models avoid real-world stumbles},
	language = {en},
	urldate = {2024-02-26},
}

@book{wooldridge_brief_2021,
  title={A Brief History of Artificial Intelligence: What It Is, Where We Are, and Where We Are Going},
  author={Wooldridge, M.},
  isbn={9781250770738},
  url={https://books.google.co.uk/books?id=5MDiDwAAQBAJ},
  year={2021},
  publisher={Flatiron Books}
}

@misc{noauthor_how_nodate,
	title = {How fast is AI improving?},
	url = {https://theaidigest.org/},
	abstract = {A visual explainer on the rate and risks of AI progress},
	language = {en},
	urldate = {2024-02-26},
	journal = {AI Digest},
}

@article{noauthor_-context_nodate,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{brown_language_2020,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{kadavath_language_2022,
  title={Language models (mostly) know what they know},
  author={Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Hatfield-Dodds, Zac and DasSarma, Nova and Tran-Johnson, Eli and others},
  journal={arXiv preprint arXiv:2207.05221},
  year={2022}
}


@article{kosinski_evaluating_2024,
  title={Evaluating Large Language Models in Theory of Mind Tasks},
  author={Kosinski, Michal},
  journal={arXiv e-prints},
  pages={arXiv--2302},
  year={2023}
}

@article{xu_opentom_2024,
  title={OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models},
  author={Xu, Hainiu and Zhao, Runcong and Zhu, Lixing and Du, Jinhua and He, Yulan},
  journal={arXiv preprint arXiv:2402.06044},
  year={2024}
}

@article{qin_toolllm_2023,
  title={Toolllm: Facilitating large language models to master 16000+ real-world apis},
  author={Qin, Yujia and Liang, Shihao and Ye, Yining and Zhu, Kunlun and Yan, Lan and Lu, Yaxi and Lin, Yankai and Cong, Xin and Tang, Xiangru and Qian, Bill and others},
  journal={arXiv preprint arXiv:2307.16789},
  year={2023}
}

@article{shinn_reflexion_2023,
  title={Reflexion: Language agents with verbal reinforcement learning},
  author={Shinn, Noah and Cassano, Federico and Gopinath, Ashwin and Narasimhan, Karthik and Yao, Shunyu},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{bubeck_sparks_2023,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{noauthor_solving_nodate,
	title={Solving olympiad geometry without human demonstrations},
	author={Trinh, Trieu H and Wu, Yuhuai and Le, Quoc V and He, He and Luong, Thang},
	journal={Nature},
	volume={625},
	number={7995},
	pages={476--482},
	year={2024},
	publisher={Nature Publishing Group}
}

@article{noauthor_gpt-4_nodate,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@inproceedings{noauthor_segment_nodate,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4015--4026},
  year={2023}
}

@article{goodfellow_generative_2014,
  title={Generative adversarial networks},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  journal={Communications of the ACM},
  volume={63},
  number={11},
  pages={139--144},
  year={2020},
  publisher={ACM New York, NY, USA}
}

@article{radford_unsupervised_2016,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv preprint arXiv:1511.06434},
  year={2015}
}

@article{liu_coupled_2016,
  title={Coupled generative adversarial networks},
  author={Liu, Ming-Yu and Tuzel, Oncel},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{karras_progressive_2018,
  title={Progressive growing of gans for improved quality, stability, and variation},
  author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  journal={arXiv preprint arXiv:1710.10196},
  year={2017}
}

@misc{noauthor_how_nodate-1,
	title = {How Midjourney Evolved Over Time (Comparing V1 to V6 Outputs)},
	url = {https://goldpenguin.org/blog/midjourney-v1-to-v6-evolution/},
	urldate = {2024-02-26},
}

@article{alayrac_flamingo_2022,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{reed_generalist_2022,
  title={A generalist agent},
  author={Reed, Scott and Zolna, Konrad and Parisotto, Emilio and Colmenarejo, Sergio Gomez and Novikov, Alexander and Barth-Maron, Gabriel and Gimenez, Mai and Sulsky, Yury and Kay, Jackie and Springenberg, Jost Tobias and others},
  journal={arXiv preprint arXiv:2205.06175},
  year={2022}
}

@article{gemini_team_gemini_2023,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{smith_walk_2022,
  title={A walk in the park: Learning to walk in 20 minutes with model-free reinforcement learning},
  author={Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  journal={arXiv preprint arXiv:2208.07860},
  year={2022}
}

@article{driess_palm-e_2023,
  title={Palm-e: An embodied multimodal language model},
  author={Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
  journal={arXiv preprint arXiv:2303.03378},
  year={2023}
}

@article{noauthor_rt-2_nodate,
  title={Rt-2: Vision-language-action models transfer web knowledge to robotic control},
  author={Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and others},
  journal={arXiv preprint arXiv:2307.15818},
  year={2023}
}

@article{fu_mobile_2024,
  title={Mobile aloha: Learning bimanual mobile manipulation with low-cost whole-body teleoperation},
  author={Fu, Zipeng and Zhao, Tony Z and Finn, Chelsea},
  journal={arXiv preprint arXiv:2401.02117},
  year={2024}
}

@misc{noauthor_deep_2024,
	title = {Deep Blue (chess computer)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Deep_Blue_(chess_computer)&oldid=1210343921},
	abstract = {Deep Blue was a chess-playing expert system run on a unique purpose-built IBM supercomputer. It was the first computer to win a game, and the first to win a match, against a reigning world champion under regular time controls. Development began in 1985 at Carnegie Mellon University under the name ChipTest. It then moved to IBM, where it was first renamed Deep Thought, then again in 1989 to Deep Blue. It first played world champion Garry Kasparov in a six-game match in 1996, where it lost four games to two. It was upgraded in 1997 and in a six-game re-match, it defeated Kasparov by winning two games and drawing three. Deep Blue's victory is considered a milestone in the history of artificial intelligence and has been the subject of several books and films.},
	language = {en},
	urldate = {2024-02-26},
	journal = {Wikipedia},
	month = feb,
	year = {2024},
	note = {Page Version ID: 1210343921},
}

@misc{noauthor_scrabble_nodate,
	title = {Scrabble Showdown: Quackle vs. David Boys - Top 10 Man-vs.-Machine Moments - TIME},
	url = {https://content.time.com/time/specials/packages/article/0,28804,2049187_2049195_2049083,00.html},
	urldate = {2024-02-26},
}

@article{webley_top_2011,
	title = {Top 10 Man-vs.-Machine Moments - TIME},
	issn = {0040-781X},
	url = {https://content.time.com/time/specials/packages/article/0,28804,2049187_2049195_2049083,00.html},
	abstract = {Using words like qadi (a Muslim judge), anuria (the nonpassage of urine) and alif (a type of spinal fusion), a software program called Quackle beat David Boys 482-465 in the final round of the 2006...},
	language = {en-US},
	urldate = {2024-02-26},
	journal = {Time},
	author = {Webley, Kayla},
	month = feb,
	year = {2011},
}

@article{noauthor_alphago_2020,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{noauthor_alphazero_nodate,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}

@article{mnih_playing_2013,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@article{noauthor_openai_nodate,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{noauthor_alphastar_nodate,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@article{noauthor_muzero_2020,
  title={Mastering atari, go, chess and shogi by planning with a learned model},
  author={Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and others},
  journal={Nature},
  volume={588},
  number={7839},
  pages={604--609},
  year={2020},
  publisher={Nature Publishing Group UK London}
}

@article{bakhtin_mastering_2022,
  title={Mastering the game of no-press Diplomacy via human-regularized reinforcement learning and planning},
  author={Bakhtin, Anton and Wu, David J and Lerer, Adam and Gray, Jonathan and Jacob, Athul Paul and Farina, Gabriele and Miller, Alexander H and Brown, Noam},
  journal={arXiv preprint arXiv:2210.05492},
  year={2022}
}

@article{noauthor_expert_nodate,
  title={Human-level play in the game of Diplomacy by combining language models with strategic reasoning},
  author={Meta Fundamental AI Research Diplomacy Team (FAIR)† and Bakhtin, Anton and Brown, Noam and Dinan, Emily and Farina, Gabriele and Flaherty, Colin and Fried, Daniel and Goff, Andrew and Gray, Jonathan and Hu, Hengyuan and others},
  journal={Science},
  volume={378},
  number={6624},
  pages={1067--1074},
  year={2022},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{radford_robust_2022,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}

@article{noauthor_230516291_nodate,
  title={Voyager: An open-ended embodied agent with large language models},
  author={Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  journal={arXiv preprint arXiv:2305.16291},
  year={2023}
}

@article{bommasani_opportunities_2022,
  title={On the opportunities and risks of foundation models},
  author={Bommasani, Rishi and Hudson, Drew A and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and others},
  journal={arXiv preprint arXiv:2108.07258},
  year={2021}
}

@misc{noauthor_linkedin_nodate,
	title = {Altman to Gates: "Multimodality will be important"},
	url = {https://www.linkedin.com/pulse/altman-multimodality-important-david-cronshaw-5fz0c},
	urldate = {2024-02-26},
}

@article{legg_formal_2006,
  title={A formal measure of machine intelligence},
  author={Legg, Shane and Hutter, Marcus},
  journal={arXiv preprint cs/0605024},
  year={2006}
}

@article{chollet_measure_2019,
  title={On the measure of intelligence},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1911.01547},
  year={2019}
}

@misc{noauthor_when_nodate,
	title = {When discussing AI risks, talk about capabilities, not intelligence},
	url = {https://www.alignmentforum.org/posts/JtuTQgp9Wnd6R6F5s/when-discussing-ai-risks-talk-about-capabilities-not},
	urldate = {2024-02-26},
}

@article{legg_universal_2007,
  title={Universal intelligence: A definition of machine intelligence},
  author={Legg, Shane and Hutter, Marcus},
  journal={Minds and machines},
  volume={17},
  pages={391--444},
  year={2007},
  publisher={Springer}
}

@article{noauthor_updates_nodate,
  title={Updates to the OECD’s definition of an AI system explained},
  author={Russell, Stuart},
  year={2023},
  publisher={OECD: Organisation for Economic Co-operation and Development}
}

@misc{noauthor_what_nodate,
	title = {What is Artificial Intelligence?},
	url = {https://www.ibm.com/topics/artificial-intelligence},
	urldate = {2024-02-26},
}

@misc{noauthor_what_nodate-1,
	title = {What is AGI?},
	url = {https://intelligence.org/2013/08/11/what-is-agi/},
	urldate = {2024-02-26},
}

@misc{muehlhauser_what_2013,
	title = {What is Intelligence?},
	url = {https://intelligence.org/2013/06/19/what-is-intelligence-2/},
	abstract = {When asked their opinions about “human-level artificial intelligence” — aka “artificial general intelligence” (AGI)1 — many experts understandably reply that these terms haven’t yet been precisely defined, and it’s hard to talk about something that hasn’t been defined.2 In this post, I want to briefly outline an imprecise but useful “working definition” for intelligence we... Read more »},
	language = {en-US},
	urldate = {2024-02-26},
	journal = {Machine Intelligence Research Institute},
	author = {Muehlhauser, Luke},
	month = jun,
	year = {2013},
}

@misc{noauthor_background_nodate,
	title = {Some Background on Our Views Regarding Advanced Artificial Intelligence},
	url = {https://www.openphilanthropy.org/research/some-background-on-our-views-regarding-advanced-artificial-intelligence/},
	abstract = {We’re planning to make potential risks from advanced artificial intelligence a major priority in 2016. A future post will discuss why; this post gives some background. Summary: I first give our definition of “transformative artificial intelligence,” our term for a type of potential advanced artificial intelligence we find particularly relevant for our purposes. Roughly and […]},
	language = {en-us},
	urldate = {2024-02-26},
	journal = {Open Philanthropy},
}

@misc{noauthor_x_nodate,
	title = {The gears are numbered 1 to 7 around the circle.},
	url = {https://twitter.com/ylecun/status/1639696127132835840},
	urldate = {2024-02-26},
}

@article{greenblatt_ai_2024,
  title={Ai control: Improving safety despite intentional subversion},
  author={Greenblatt, Ryan and Shlegeris, Buck and Sachan, Kshitij and Roger, Fabien},
  journal={arXiv preprint arXiv:2312.06942},
  year={2023}
}

@misc{noauthor_moores_2024,
	title = {Moore's law},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Moore%27s_law&oldid=1210210867},
	language = {en},
	urldate = {2024-02-26},
	journal = {Wikipedia},
	month = feb,
	year = {2024},
	note = {Page Version ID: 1210210867},
}

@article{noauthor_bitter_nodate,
  title={The bitter lesson},
  author={Sutton, Richard},
  journal={Incomplete Ideas (blog)},
  volume={13},
  number={1},
  pages={38},
  year={2019}
}

@article{noauthor_parti_nodate,
  title={Scaling autoregressive models for content-rich text-to-image generation},
  author={Yu, Jiahui and Xu, Yuanzhong and Koh, Jing Yu and Luong, Thang and Baid, Gunjan and Wang, Zirui and Vasudevan, Vijay and Ku, Alexander and Yang, Yinfei and Ayan, Burcu Karagol and others},
  journal={arXiv preprint arXiv:2206.10789},
  volume={2},
  number={3},
  pages={5},
  year={2022}
}


@article{noauthor_what_nodate-2,
	title = {What DALL-E 2 can and cannot do},
	url = {https://www.lesswrong.com/posts/uKp6tBFStnsvrot5t/what-dall-e-2-can-and-cannot-do},
	urldate = {2024-02-26},
}

@article{noauthor_new_nodate,
	title = {New Scaling Laws for Large Language Models},
	url = {https://www.lesswrong.com/posts/midXmMb2Xg37F2Kgn/new-scaling-laws-for-large-language-models},
	urldate = {2024-02-26},
  authors = {1a3orn},
  year = {2022}
}

@article{kaplan_scaling_2020,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@misc{noauthor_scaling_nodate,
	title = {The Scaling Hypothesis},
  author = {Gwern},
	url = {https://gwern.net/scaling-hypothesis#scaling-hypothesis},
	urldate = {2024-02-26},
}

@article{lecun_path_nodate,
  title={A path towards autonomous machine intelligence},
  author={LeCun, Yann},
  journal={Open Review},
  volume={62},
  number={1},
  year={2022}
}

@article{sutton_alberta_2023,
  title={The Alberta plan for AI research},
  author={Sutton, Richard S and Bowling, Michael and Pilarski, Patrick M},
  journal={arXiv preprint arXiv:2208.11173},
  year={2022}
}


@misc{noauthor_planning_nodate,
	title = {Planning for AGI and beyond},
	url = {https://openai.com/blog/planning-for-agi-and-beyond},
	abstract = {Our mission is to ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity.},
	language = {en-US},
	urldate = {2024-02-26},
}

@misc{noauthor_introducing_nodate,
	title = {Introducing Superalignment},
	url = {https://openai.com/blog/introducing-superalignment},
	abstract = {We need scientific and technical breakthroughs to steer and control AI systems much smarter than us. To solve this problem within four years, we’re starting a new team, co-led by Ilya Sutskever and Jan Leike, and dedicating 20\% of the compute we’ve secured to date to this effort. We’re looking for excellent ML researchers and engineers to join us.},
	language = {en-US},
	urldate = {2024-02-26},
}

@misc{dwarkesh_patel_dario_2023,
	title = {Dario Amodei (Anthropic CEO) - $10 Billion Models, OpenAI, Scaling, & Alignment},
	url = {https://www.youtube.com/watch?v=Nlkk3glap_U},
	abstract = {Here is my conversation with Dario Amodei, CEO of Anthropic.},
	urldate = {2024-02-26},
	author = {{Dwarkesh Patel}},
	month = aug,
	year = {2023},
}

@article{zac_kenton_clarifying_nodate,
	title = {Clarifying AI X-risk},
	url = {https://www.lesswrong.com/posts/GctJD5oCDRxCspEaZ/clarifying-ai-x-risk},
	abstract = {TL;DR: We give a threat model literature review, propose a categorization and describe a consensus threat model from some of DeepMind's AGI safety te…},
	language = {en},
	urldate = {2024-02-26},
	author = {zac_kenton and Shah, Rohin and Lindner, David and Varma, Vikrant and Vika and Phuong, Mary and Kumar, Ramana and Catt, Elliot},
}

@article{andrea_miotti_agi_nodate,
	title = {AGI in sight: our look at the game board},
	url = {https://www.lesswrong.com/posts/PE22QJSww8mpwh7bt/agi-in-sight-our-look-at-the-game-board},
	abstract = {From our point of view, we are now in the end-game for AGI, and we (humans) are losing. When we share this with other people, they reliably get surpr…},
	language = {en},
	urldate = {2024-02-26},
	author = {Andrea_Miotti and Alfour, Gabriel},
}

@misc{noauthor_forecasting_nodate,
	title = {Forecasting: Lecture Notes - 5 Zeroth and First Order Forecasting},
	url = {https://forecasting.quarto.pub/book/zeroth-first.html?ref=bounded-regret.ghost.io#first-order-approximation},
	urldate = {2024-02-26},
}

@misc{noauthor_what_2023,
	title = {What will GPT-2030 look like?},
	url = {https://bounded-regret.ghost.io/what-will-gpt-2030-look-like/},
	abstract = {GPT-4 surprised many people with its abilities at coding, creative brainstorming, letter-writing, and other skills. How can we be less surprised by developments in machine learning? In this post, I’ll forecast the properties of large pretrained ML systems in 2030.},
	language = {en},
	urldate = {2024-02-26},
	journal = {Bounded Regret},
	month = jun,
	year = {2023},
}

@misc{noauthor_biological_nodate,
	title = {“Biological anchors” is about bounding, not pinpointing},
	url = {https://forum.effectivealtruism.org/posts/ajBYeiggAzu6Cgb3o/biological-anchors-is-about-bounding-not-pinpointing-ai},
	urldate = {2024-02-26},
}

@misc{ho_grokking_2022,
	title = {Grokking “Forecasting TAI With Biological Anchors”},
	url = {https://epochai.org/blog/grokking-bioanchors},
	abstract = {I give a visual explanation of Ajeya Cotra’s draft report, Forecasting TAI with biological anchors, summarising the key assumptions, intuitions, and conclusions.},
	language = {en},
	urldate = {2024-02-26},
	journal = {Epoch},
	author = {Ho, Anson},
	month = jun,
	year = {2022},
}

@misc{noauthor_simple_nodate,
	title = {Simple evolution analogy},
	url = {https://docs.google.com/document/d/1HUtUBpRbNnnWBxiO2bz3LumEsQcaZioAPZDNcsWPnos/edit?usp=embed_facebook},
	language = {fr},
	urldate = {2024-02-26},
	journal = {Google Docs},
}

@article{cotra_draft_nodate,
	title = {Draft report on AI timelines},
	url = {https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines},
	abstract = {Hi all, I've been working on some AI forecasting research and have prepared a draft report on timelines to transformative AI. I would love feedback f…},
	language = {en},
	urldate = {2024-02-26},
	author = {Cotra, Ajeya},
}

@misc{noauthor_ai_nodate,
	title = {AI and compute},
	url = {https://openai.com/research/ai-and-compute},
	abstract = {We’re releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore’s Law had a 2-year doubling period)[{\textasciicircum}footnote-correction]. Since 2012, this metric has grown by more than 300,000x (a 2-year doubling period would yield only a 7x increase). Improvements in compute have been a key component of AI progress, so as long as this trend continues, it’s worth preparing for the implications of systems far outside today’s capabilities.},
	language = {en-US},
	urldate = {2024-02-26},
}

@misc{noauthor_two-year_nodate,
	title = {Two-year update on my personal AI timelines},
	url = {https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines},
	urldate = {2024-02-26},
}

@misc{noauthor_biology-inspired_nodate,
	title = {Biology-Inspired AGI Timelines: The Trick That Never Works},
	url = {https://intelligence.org/2021/12/03/biology-inspired-agi-timelines-the-trick-that-never-works/},
	urldate = {2024-02-26},
}

@misc{noauthor_reply_nodate,
	title = {Reply to Eliezer on Biological Anchors},
	url = {https://www.alignmentforum.org/posts/nNqXfnjiezYukiMJi/reply-to-eliezer-on-biological-anchors},
	urldate = {2024-02-26},
}

@article{martin_takeoff_nodate,
	title = {Takeoff Speeds and Discontinuities},
	url = {https://www.alignmentforum.org/posts/pGXR2ynhe5bBCCNqn/takeoff-speeds-and-discontinuities},
	abstract = {This post is part 4 in our sequence on Modeling Transformative AI Risk. We are building a model to understand debates around existential risks from a…},
	language = {en},
	urldate = {2024-02-26},
	author = {Martin, Sammy and Daniel\_Eth},
}

@misc{noauthor_homogeneity_nodate,
	title = {Homogeneity vs. heterogeneity in AI takeoff scenarios},
	url = {https://www.lesswrong.com/posts/mKBfa8v4S9pNKSyKK/homogeneity-vs-heterogeneity-in-ai-takeoff-scenarios},
	urldate = {2024-02-26},
}

@article{yudkowsky_intelligence_2013,
  title={Intelligence explosion microeconomics},
  author={Yudkowsky, Eliezer},
  journal={Machine Intelligence Research Institute, accessed online October},
  volume={23},
  pages={2015},
  year={2013}
}

@incollection{muehlhauser_intelligence_2012,
  title={Intelligence explosion: Evidence and import},
  author={Muehlhauser, Luke and Salamon, Anna},
  booktitle={Singularity hypotheses: A scientific and philosophical assessment},
  pages={15--42},
  year={2013},
  publisher={Springer}
}

@article{noauthor_what_nodate-3,
	title = {What a Compute-Centric Framework Says About Takeoff Speeds},
	url = {https://www.openphilanthropy.org/research/what-a-compute-centric-framework-says-about-takeoff-speeds/},
	abstract = {This is Part 0 of a four-part report — see links to Part 1. Part 2. Part 3, and a folder with more materials. Abstract In the next few decades we may develop AI that can automate {\textasciitilde}all cognitive tasks and dramatically transform the world. By contrast, today the capabilities and impact of AI are much […]},
	language = {en-us},
	urldate = {2024-02-26},
	journal = {Open Philanthropy},
}

@article{hoffmann_training_2022,
  title={Training compute-optimal large language models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}


@misc{noauthor_forecasting_nodate-1,
	title = {Forecasting transformative AI: the "biological anchors" method in a nutshell},
	url = {https://forum.effectivealtruism.org/posts/vCaEnTbZ5KbypaGsm/forecasting-transformative-ai-the-biological-anchors-method},
	urldate = {2024-02-26},
}

@article{grace2024thousands,
  title={Thousands of AI authors on the future of AI},
  author={Grace, Katja and Stewart, Harlan and Sandk{\"u}hler, Julia Fabienne and Thomas, Stephen and Weinstein-Raun, Ben and Brauner, Jan},
  journal={arXiv preprint arXiv:2401.02843},
  year={2024}
}

@article{godfather,
  title={"Godfather of artificial intelligence" weighs in on the past and potential of AI},
  url={https://www.cbsnews.com/news/godfather-of-artificial-intelligence-weighs-in-on-the-past-and-potential-of-artificial-intelligence/},
  year={2023},
  journal={CBS News}
} 

@article{bengio,
  title={Personal and Psychological Dimensions of AI Researchers Confronting AI Catastrophic Risks},
  url={https://yoshuabengio.org/2023/08/12/personal-and-psychological-dimensions-of-ai-researchers-confronting-ai-catastrophic-risks/},
  year={2023},
  author={Bengio, Yoshua}
}

@article{lecun,
  title={Personal and Psychological Dimensions of AI Researchers Confronting AI Catastrophic RisksMeta’s AI chief doesn’t think AI super intelligence is coming anytime soon, and is skeptical on quantum computing},
  url={https://www.cnbc.com/2023/12/03/meta-ai-chief-yann-lecun-skeptical-about-agi-quantum-computing.html},
  year={2023},
  author={Vanian, Jonathan}
}

@article{ilya,
  title={OpenAI's Chief Scientist Ilya Sutskever comments on Artificial General Intelligence},
  url={https://old.reddit.com/r/singularity/comments/kxgg1b/openais_chief_scientist_ilya_sutskever_comments/},
}

@article{demis,
  title={The Race for True AI at Google},
  url={https://www.wsj.com/video/events/the-race-for-true-ai-at-google/7953FE4B-AE84-4AFA-9722-AA215EB357EE.html},
  year={2023},
}

@article{timelines,
  title={The Race for True AI at Google},
  url={https://www.reddit.com/r/singularity/comments/18vawje/comment/kfpntso/},
  year={2024},
}

@article{dreber2015using,
  title={Using prediction markets to estimate the reproducibility of scientific research},
  author={Dreber, Anna and Pfeiffer, Thomas and Almenberg, Johan and Isaksson, Siri and Wilson, Brad and Chen, Yiling and Nosek, Brian A and Johannesson, Magnus},
  journal={Proceedings of the National Academy of Sciences},
  volume={112},
  number={50},
  pages={15343--15347},
  year={2015},
  publisher={National Acad Sciences}
}

@article{metaculus,
  title={When will the first general AI system be devised, tested, and publicly announced?},
  url={https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/},
  year={2024},
}

@article{romera2024mathematical,
  title={Mathematical discoveries from program search with large language models},
  author={Romera-Paredes, Bernardino and Barekatain, Mohammadamin and Novikov, Alexander and Balog, Matej and Kumar, M Pawan and Dupont, Emilien and Ruiz, Francisco JR and Ellenberg, Jordan S and Wang, Pengming and Fawzi, Omar and others},
  journal={Nature},
  volume={625},
  number={7995},
  pages={468--475},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@article{mcgrath2022acquisition,
  title={Acquisition of chess knowledge in alphazero},
  author={McGrath, Thomas and Kapishnikov, Andrei and Toma{\v{s}}ev, Nenad and Pearce, Adam and Wattenberg, Martin and Hassabis, Demis and Kim, Been and Paquet, Ulrich and Kramnik, Vladimir},
  journal={Proceedings of the National Academy of Sciences},
  volume={119},
  number={47},
  pages={e2206625119},
  year={2022},
  publisher={National Acad Sciences}
},

@article{specification,
  title={Specification gaming: the flip side of AI ingenuity},
  author={Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik, Matthew Rahtz, Tom Everitt, Ramana Kumar, Zac Kenton, Jan Leike, Shane Legg},
  url={https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/},
  year={2020},
},

@article{ye2021mastering,
  title={Mastering atari games with limited data},
  author={Ye, Weirui and Liu, Shaohuai and Kurutach, Thanard and Abbeel, Pieter and Gao, Yang},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={25476--25488},
  year={2021}
},

@article{timeforai,
  title={Time for AI to cross the human performance range in ImageNet image classification},
  url={https://aiimpacts.org/time-for-ai-to-cross-the-human-performance-range-in-imagenet-image-classification/},
  year={2020}
},

@article{fchollet,
  title={Unfortunately , too few people understand the distinction between memorization and understanding},
  url={https://twitter.com/fchollet/status/1735799743505433020},
  year={2023}
},

@article{fchollet2,
  title={Ok, by popular demand: a starter set of papers you can read on the topic.},
  url={https://twitter.com/fchollet/status/1736079054313574578},
  year={2023}
},

@article{charbel,
  title={Here is a point by point answer.},
  url={https://twitter.com/CRSegerie/status/1736466297175560268},
  year={2023}
},

@article{elhage2022toy,
  title={Toy models of superposition},
  author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and others},
  journal={arXiv preprint arXiv:2209.10652},
  year={2022}
},

@article{mishra2023promptaid,
  title={Promptaid: Prompt exploration, perturbation, testing and iteration using visual analytics for large language models},
  author={Mishra, Aditi and Soni, Utkarsh and Arunkumar, Anjana and Huang, Jinbin and Kwon, Bum Chul and Bryan, Chris},
  journal={arXiv preprint arXiv:2304.01964},
  year={2023}
},

@article{meng2022locating,
  title={Locating and editing factual associations in GPT},
  author={Meng, Kevin and Bau, David and Andonian, Alex and Belinkov, Yonatan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={17359--17372},
  year={2022}
},

@article{nanda2023emergent,
  title={Emergent linear representations in world models of self-supervised sequence models},
  author={Nanda, Neel and Lee, Andrew and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2309.00941},
  year={2023}
},

@article{mishra2023promptaid,
  title={Promptaid: Prompt exploration, perturbation, testing and iteration using visual analytics for large language models},
  author={Mishra, Aditi and Soni, Utkarsh and Arunkumar, Anjana and Huang, Jinbin and Kwon, Bum Chul and Bryan, Chris},
  journal={arXiv preprint arXiv:2304.01964},
  year={2023}
},

@article{quentin,
  title={The Stochastic Parrot Hypothesis is debatable for the last generation of LLMs},
  author={Quentin FEUILLADE--MONTIXI, Pierre Peigné},
  url={https://www.lesswrong.com/posts/HxRjHq3QG8vcYy4yy/the-stochastic-parrot-hypothesis-is-debatable-for-the-last},
  year={2023}
},

@article{tian2023fine,
  title={Fine-tuning language models for factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2311.08401},
  year={2023}
},

@misc{fluri2023evaluating,
      title={Evaluating Superhuman Models with Consistency Checks}, 
      author={Lukas Fluri and Daniel Paleka and Florian Tramèr},
      year={2023},
      eprint={2306.09983},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},

@misc{shinn2023reflexion,
      title={Reflexion: Language Agents with Verbal Reinforcement Learning}, 
      author={Noah Shinn and Federico Cassano and Edward Berman and Ashwin Gopinath and Karthik Narasimhan and Shunyu Yao},
      year={2023},
      eprint={2303.11366},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
},

@misc{creswell2022selectioninference,
      title={Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning}, 
      author={Antonia Creswell and Murray Shanahan and Irina Higgins},
      year={2022},
      eprint={2205.09712},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
},

@misc{rae2022scaling,
      title={Scaling Language Models: Methods, Analysis & Insights from Training Gopher}, 
      author={Jack W. Rae and Sebastian Borgeaud and Trevor Cai and Katie Millican and Jordan Hoffmann and Francis Song and John Aslanides and Sarah Henderson and Roman Ring and Susannah Young and Eliza Rutherford and Tom Hennigan and Jacob Menick and Albin Cassirer and Richard Powell and George van den Driessche and Lisa Anne Hendricks and Maribeth Rauh and Po-Sen Huang and Amelia Glaese and Johannes Welbl and Sumanth Dathathri and Saffron Huang and Jonathan Uesato and John Mellor and Irina Higgins and Antonia Creswell and Nat McAleese and Amy Wu and Erich Elsen and Siddhant Jayakumar and Elena Buchatskaya and David Budden and Esme Sutherland and Karen Simonyan and Michela Paganini and Laurent Sifre and Lena Martens and Xiang Lorraine Li and Adhiguna Kuncoro and Aida Nematzadeh and Elena Gribovskaya and Domenic Donato and Angeliki Lazaridou and Arthur Mensch and Jean-Baptiste Lespiau and Maria Tsimpoukelli and Nikolai Grigorev and Doug Fritz and Thibault Sottiaux and Mantas Pajarskas and Toby Pohlen and Zhitao Gong and Daniel Toyama and Cyprien de Masson d'Autume and Yujia Li and Tayfun Terzi and Vladimir Mikulik and Igor Babuschkin and Aidan Clark and Diego de Las Casas and Aurelia Guy and Chris Jones and James Bradbury and Matthew Johnson and Blake Hechtman and Laura Weidinger and Iason Gabriel and William Isaac and Ed Lockhart and Simon Osindero and Laura Rimell and Chris Dyer and Oriol Vinyals and Kareem Ayoub and Jeff Stanway and Lorrayne Bennett and Demis Hassabis and Koray Kavukcuoglu and Geoffrey Irving},
      year={2022},
      eprint={2112.11446},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},

@misc{lightman2023lets,
      title={Let's Verify Step by Step}, 
      author={Hunter Lightman and Vineet Kosaraju and Yura Burda and Harri Edwards and Bowen Baker and Teddy Lee and Jan Leike and John Schulman and Ilya Sutskever and Karl Cobbe},
      year={2023},
      eprint={2305.20050},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},

@misc{kadavath2022language,
      title={Language Models (Mostly) Know What They Know}, 
      author={Saurav Kadavath and Tom Conerly and Amanda Askell and Tom Henighan and Dawn Drain and Ethan Perez and Nicholas Schiefer and Zac Hatfield-Dodds and Nova DasSarma and Eli Tran-Johnson and Scott Johnston and Sheer El-Showk and Andy Jones and Nelson Elhage and Tristan Hume and Anna Chen and Yuntao Bai and Sam Bowman and Stanislav Fort and Deep Ganguli and Danny Hernandez and Josh Jacobson and Jackson Kernion and Shauna Kravec and Liane Lovitt and Kamal Ndousse and Catherine Olsson and Sam Ringer and Dario Amodei and Tom Brown and Jack Clark and Nicholas Joseph and Ben Mann and Sam McCandlish and Chris Olah and Jared Kaplan},
      year={2022},
      eprint={2207.05221},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},

@misc{delétang2023neural,
      title={Neural Networks and the Chomsky Hierarchy}, 
      author={Grégoire Delétang and Anian Ruoss and Jordi Grau-Moya and Tim Genewein and Li Kevin Wenliang and Elliot Catt and Chris Cundy and Marcus Hutter and Shane Legg and Joel Veness and Pedro A. Ortega},
      year={2023},
      eprint={2207.02098},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},

@misc{gurnee2024language,
      title={Language Models Represent Space and Time}, 
      author={Wes Gurnee and Max Tegmark},
      year={2024},
      eprint={2310.02207},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},

@misc{nanda2023emergent,
      title={Emergent Linear Representations in World Models of Self-Supervised Sequence Models}, 
      author={Neel Nanda and Andrew Lee and Martin Wattenberg},
      year={2023},
      eprint={2309.00941},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},

@misc{bowman2023things,
      title={Eight Things to Know about Large Language Models}, 
      author={Samuel R. Bowman},
      year={2023},
      eprint={2304.00612},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
},

@article{jacob,
  title={AI Timelines via Cumulative Optimization Power: Less Long, More Short},
  author={jacob_cannell},
  url={https://www.lesswrong.com/posts/3nMpdmt8LrzxQnkGp/ai-timelines-via-cumulative-optimization-power-less-long#fn-uL4CtAHDBwDHrweh8-4},
  year={2022}
},

@article{caucheteux2022brains,
  title={Brains and algorithms partially converge in natural language processing},
  author={Caucheteux, Charlotte and King, Jean-R{\'e}mi},
  journal={Communications biology},
  volume={5},
  number={1},
  pages={134},
  year={2022},
  publisher={Nature Publishing Group UK London}
},

@article{jacob2,
  title={The Brain as a Universal Learning Machine},
  author={jacob_cannell},
  url={https://www.lesswrong.com/posts/9Yc7Pp7szcjPgPsjf/the-brain-as-a-universal-learning-machine},
  year={2015}
},

@article{jacob3,
  title={Brain Efficiency: Much More than You Wanted to Know},
  author={jacob_cannell},
  url={https://www.lesswrong.com/posts/xwBuoE9p8GE7RAuhd/brain-efficiency-much-more-than-you-wanted-to-know},
  year={2022}
},

@article{schrimpf2021neural,
  title={The neural architecture of language: Integrative modeling converges on predictive processing},
  author={Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A and Kanwisher, Nancy and Tenenbaum, Joshua B and Fedorenko, Evelina},
  journal={Proceedings of the National Academy of Sciences},
  volume={118},
  number={45},
  pages={e2105646118},
  year={2021},
  publisher={National Acad Sciences}
},

@article{herculano2012remarkable,
  title={The remarkable, yet not extraordinary, human brain as a scaled-up primate brain and its associated cost},
  author={Herculano-Houzel, Suzana},
  journal={Proceedings of the National Academy of Sciences},
  volume={109},
  number={supplement\_1},
  pages={10661--10668},
  year={2012},
  publisher={National Acad Sciences}
},

@article{pearson2023updated,
  title={Updated imaging and phylogenetic comparative methods reassess relative temporal lobe size in anthropoids and modern humans},
  author={Pearson, Alannah and Bruner, Emiliano and Polly, P David},
  journal={American Journal of Biological Anthropology},
  volume={180},
  number={4},
  pages={768--776},
  year={2023},
  publisher={Wiley Online Library}
},

@article{charvet2021cutting,
  title={Cutting across structural and transcriptomic scales translates time across the lifespan in humans and chimpanzees},
  author={Charvet, Christine J},
  journal={Proceedings of the Royal Society B},
  volume={288},
  number={1944},
  pages={20202987},
  year={2021},
  publisher={The Royal Society}
},

@article{ai-complete,
  title={AI-complete},
  author={Wikipedia},
  url={https://en.wikipedia.org/wiki/AI-complete},
},

@article{gpt175,
  title={GPT-175bee},
  author={Adam Scherlis, LawrenceC},
  url={https://www.lesswrong.com/posts/YKfNZAmiLdepDngwi/gpt-175bee},
  year={2023}
},

@article{buchanan2020ai,
  title={The AI triad and what it means for national security strategy},
  author={Buchanan, Ben},
  journal={Center for Security and Emerging Technology. https://cset. georgetown. edu/research/the-aitriad-and-what-it-means-for-national-security-strategy},
  year={2020}
},

@misc{shane,
  title={Goodbye 2010},
  author={Shane},
  year={2010},
  url={http://www.vetta.org/2010/12/goodbye-2010/},
},

@misc{villalobos2022machine,
      title={Machine Learning Model Sizes and the Parameter Gap}, 
      author={Pablo Villalobos and Jaime Sevilla and Tamay Besiroglu and Lennart Heim and Anson Ho and Marius Hobbhahn},
      year={2022},
      eprint={2207.02852},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},

@misc{wef,
  title={How much data is generated each day?},
  author={Jeff Desjardins},
  year={2019},
  url={https://www.weforum.org/agenda/2019/04/how-much-data-is-generated-each-day-cf4bddf29f/},
},

@article{smith2023convnets,
      title={ConvNets Match Vision Transformers at Scale}, 
      author={Samuel L. Smith and Andrew Brock and Leonard Berrada and Soham De},
      year={2023},
      eprint={2310.16764},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
},

@misc{ganwiki,
  title={Generative Adversarial Network},
  author={Wikipedia},
  url={https://en.wikipedia.org/wiki/Generative_adversarial_network},
},

@article{child2021deep,
      title={Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images}, 
      author={Rewon Child},
      year={2021},
      eprint={2011.10650},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},

@article{gu2023mamba,
      title={Mamba: Linear-Time Sequence Modeling with Selective State Spaces}, 
      author={Albert Gu and Tri Dao},
      year={2023},
      eprint={2312.00752},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},

@article{he2023deep,
  title={Deep transformers without shortcuts: Modifying self-attention for faithful signal propagation},
  author={He, Bobby and Martens, James and Zhang, Guodong and Botev, Aleksandar and Brock, Andrew and Smith, Samuel L and Teh, Yee Whye},
  journal={arXiv preprint arXiv:2302.10322},
  year={2023}
},

@article{child2019generating,
      title={Generating Long Sequences with Sparse Transformers}, 
      author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
      year={2019},
      eprint={1904.10509},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
},

@article{beltagy2020longformer,
      title={Longformer: The Long-Document Transformer}, 
      author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
      year={2020},
      eprint={2004.05150},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{epoch2023aitrends,
  title={Key Trends and Figures in Machine Learning},
  author={Epoch},
  year={2023},
  url={https://epochai.org/trends},
  note={Accessed: 2024-03-14}
}