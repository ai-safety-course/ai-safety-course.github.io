---
layout: distill
title: Chapter 0 - Artificial General Intelligence
description: an example of a distill-style blog post and main elements
#giscus_comments: true
date: 2024-01-25
featured: false

authors:
  - name: Name 1
    url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    affiliations:
      name: IAS, Princeton
  - name: Name 2
    url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
    affiliations:
      name: IAS, Princeton

bibliography: 2018-12-22-distill.bib


# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc: true
toc:
  - name: Section 1
    subsections:
    - name: Example Child Subsection 1
    - name: Example Child Subsection 2
  - name: Section 2
  - name: Others
---

Citations are then used in the article <dt-cite key="gregor2015draw"></dt-cite> body with the <d-cite key='gregor2015draw'></d-cite> tag.


# Language

Language-based tasks have seen transformative changes, primarily through the development of large language models (LLMs). The evolution from early models, which struggled to construct coherent sentences, to the advanced capabilities of GPT-3 and ChatGPT is remarkable. These models demonstrate not only an improved capacity for generating text but also for understanding and responding to complex queries with nuanced, common-sense reasoning. Their performance in various question-answering tasks, including those requiring strategic thinking, has been particularly impressive.
GPT-4 is OpenAI’s latest Large Language Model (LLM). In contrast with text-only GPT-3 and follow-ups, GPT-4 is multimodal: it was trained on both text and images; it can among other capabilities generate text based on images. Its context window has up to 32k tokens (tokens ≈ words). GPT-4 is trained via next token prediction (autoregressive self-supervised learning). GPT-1 was barely able to count to 10, GPT-4 is able to implement complex programmatic functions.
Scaling. What is remarkable is that GPT-4 is trained using roughly the same methods as GPT-1, 2, and 3. The only significant difference is the size of the model and dataset, which has gone from 1.5B parameters to hundreds of billions of parameters on trained on larger and more diverse datasets.


<div class="row mt-3">
  {% include figure.liquid path="assets/img/2_42.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  <figcaption>
        <b>Weak Regularization</b>
        avoids misleading correlations, but is less connected to real use.
  </figcaption>
</div>



# Debate

This section summarizes a line of research in which each work builds upon the previous one. The papers are presented in chronological order according to their publication date.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/1_1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid path="assets/img/1_1.png" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>



include_graphics("assets/img/l-body-1_1.png")

