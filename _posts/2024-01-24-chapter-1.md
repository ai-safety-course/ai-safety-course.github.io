---
layout: distill
title: Chapter 1 - AI Safety Foundations
description: description
#giscus_comments: true
date: 2024-01-25
featured: false

authors:
  - name: Name 1
    url: "https://en.wikipedia.org/wiki/Albert_Einstein"
    affiliations:
      name: IAS, Princeton
  - name: Name 2
    url: "https://en.wikipedia.org/wiki/Boris_Podolsky"
    affiliations:
      name: IAS, Princeton

bibliography: 2018-12-22-distill.bib

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc: true
toc:
  - name: Section 1
    subsections:
    - name: Example Child Subsection 1
    - name: Example Child Subsection 2
  - name: Section 2
  - name: Others
---

This post shows how to add bibliography to simple blog posts. We support every citation style that [jekyll-scholar](https://github.com/inukshuk/jekyll-scholar) does. That means simple citation like {% cite einstein1950meaning %}, multiple citations like {% cite einstein1950meaning einstein1905movement %}, long references like {% reference einstein1905movement %} or also quotes:

{% quote einstein1905electrodynamics %}
Lorem ipsum dolor sit amet, consectetur adipisicing elit,
sed do eiusmod tempor.

Lorem ipsum dolor sit amet, consectetur adipisicing.
{% endquote %}

If you would like something more academic, check the [distill style post]({% post_url 2018-12-22-distill %}).


This theme supports rendering beautiful math in inline and display modes using [MathJax 3](https://www.mathjax.org/) engine. You just need to surround your math expression with `$$`, like `$$ E = mc^2 $$`. If you leave it inside a paragraph, it will produce an inline expression, just like $$ E = mc^2 $$.

To use display mode, again surround your expression with `$$` and place it as a separate paragraph. Here is an example:

$$
\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2
$$

You can also use `\begin{equation}...\end{equation}` instead of `$$` for display mode math.
MathJax will automatically number equations:

\begin{equation}
\label{eq:cauchy-schwarz}
\left( \sum*{k=1}^n a_k b_k \right)^2 \leq \left( \sum*{k=1}^n a*k^2 \right) \left( \sum*{k=1}^n b_k^2 \right)
\end{equation}

and by adding `\label{...}` inside the equation environment, we can now refer to the equation using `\eqref`.

Note that MathJax 3 is [a major re-write of MathJax](https://docs.mathjax.org/en/latest/upgrading/whats-new-3.0.html) that brought a significant improvement to the loading and rendering speed, which is now [on par with KaTeX](http://www.intmath.com/cg5/katex-mathjax-comparison.php).



# Chapter Overview

**State-of-the-Art AI**. We begin with a short introduction to the current advancements in artificial intelligence as of 2024. Our aim is to acquaint readers with the latest breakthroughs across various domains such as language processing, vision, and robotics.

**Foundation Models**. The second section focuses on foundation models, the paradigm powering the state of the art systems introduced in the previous section. We explain the key-techniques underpinning the huge success of these models such as: self-supervised learning, zero shot learning, and fine-tuning. The section concludes by looking at the risks that the foundation model paradigm could pose such as power centralization, homogenization and potential for emergent capabilities.

**Terminology**. Before diving deeper, we establish the definitions that his book will be working with. This section explains why "capabilities" rather than "intelligence" is a more pragmatic measure for discussing AI risks. We also delineate key terms within the AI debate, such as Artificial General Intelligence (AGI), Artificial Super Intelligence (ASI), and Transformative AI (TAI). The section concludes by introducing the (t,n)-AGI framework which allows us to more concretely measure the level of AI capabilities on a continuous scale, rather than having to rely on discrete thresholds.

**Leveraging Computation**. In this section we explore the importance of computation in AI's progress introducing the three main variables that govern the capabilities of today's foundation models - compute, data and parameter count. We explore scaling laws and hypotheses that predict the future capabilities of AI based on current scaling trends of these variables, offering insights into the computational strategies that could pave the way to AGI.

**Forecasting**. Finally, the chapter addresses the challenge of forecasting AI's future, using biological anchors as a method to estimate the computational needs for transformative AI. This section sets the groundwork for discussing AI takeoff dynamics, including speed, polarity, and homogeneity, offering a comprehensive view of potential futures shaped by AI development.

**Estimated reading time**: 2 Hours 40 minutes reading at 100 wpm

**Estimated reading time (including Appendices)**: 3 Hours 40 minutes reading at 100 wpm

# State-of-the-Art AI

Over the last decade, the field of artificial intelligence (AI) has experienced a profound transformation, largely attributed to the successes in deep learning. This remarkable progress has redefined the boundaries of AI capabilities, challenging many preconceived notions of what machines can achieve. The following sections detail some of these advancements.

<div class="row mt-3">
  {% include figure.liquid path="assets/img/1_image32.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  <figcaption>
        <b>Figure</b>
        Once a benchmark is published, it takes less and less time to solve it. This can illustrate the accelerating progress in AI and how quickly AI benchmarks are “saturating”, and starting to surpass human performance on a variety of tasks. Source: Science article , From DynaBench.
  </figcaption>
</div>

## Language

**Language-based tasks.** There have been transformative changes in sequence and language based tasks, primarily through the development of large language models (LLMs). Early language models in 2018 struggled to construct coherent sentences. The evolution from these to the advanced capabilities of GPT-3 (Generative Pre-Trained Transformer) and ChatGPT within less than 5 years is remarkable. These models demonstrate not only an improved capacity for generating text, but also for responding to complex queries with nuanced, common-sense reasoning. Their performance in various question-answering tasks, including those requiring strategic thinking, has been particularly impressive.

**GPT-4.** One of the state of the art language models in 2024 is OpenAI’s LLM GPT-4. In contrast with the text-only GPT-3 and follow-ups, GPT-4 is multimodal: it was trained on both text and images. This means that it can now not only generate text based on images, but it has also gained some other capabilities. GPT-4 saw an upgraded context window with up to 32k tokens (tokens ≈ words). The short-term memory limit of an LLM can be thought of as the model's ability to retain information from previous tokens within a certain context window. GPT-4 is trained via next token prediction (autoregressive self-supervised learning). In 2018 GPT-1 was barely able to count to 10, while in 2024 GPT-4 is able to implement complex programmatic functions among other things.

<div class="row mt-3">
  {% include figure.liquid path="assets/img/1_image37.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  <figcaption>
        <b>Figure</b>
        A list of "Nowhere near solved” [...] issues in AI, from "A brief history of AI", published in January 2021 (source). They also say: “at present, we have no idea how to get computers to do the tasks at the bottom of the list”. But everything in the category “Nowhere near solved” has been solved by GPT-4 (source), except human-level general intelligence.
  </figcaption>
</div>

**Scaling**. Remarkably, GPT-4 is trained using roughly the same methods as GPT-1, 2, and 3. The only significant difference is the size of the model and data given to it during training. The size of the model has gone from 1.5B parameters to hundreds of billions of parameters, and datasets have become similarly larger and more diverse.

<div class="row mt-3">
  {% include figure.liquid path="assets/img/1_image53.png" class="img-fluid rounded z-depth-1" zoomable=true %}
  <figcaption>
        <b>Figure</b>
        How fast is AI Improving?
  </figcaption>
</div>

We have observed that just an expansion in scale has contributed to enhanced performance. This includes improvements in the ability to generate contextually appropriate responses, and highly diverse text across a range of domains. It has also contributed to overall improved understanding, and coherence. Most of those advances in the GPT series come from increasing the size and computation power behind the models, rather than fundamental shifts in architecture or training.

Here are some of the capabilities that are emerging in the last few years:

* **Few-shot and Zero-shot Learning**. The model's proficiency at understanding and executing tasks with minimal or no prior examples. 'Few-shot' means accomplishing the task after having seen a few examples in the context window, while 'Zero-shot' indicates performing the task without any specific examples ([source](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)). This also includes induction Capabilities, i.e. identifying patterns and generalizing rules not present in the training, but only present in the current context window ([source](https://arxiv.org/abs/2005.14165)).
* **Metacognition**. This refers to the ability to recognize its own knowledge and limitations, for example, being able to know the probability of the truth of something ([source](https://arxiv.org/abs/2207.05221)).
* **Theory of Mind**. The capability to attribute mental states to itself and others, which helps in predicting human behaviors and responses for more nuanced interactions ([source](https://arxiv.org/abs/2302.02083)).
* **Tool Use**. Being able to interact with external tools, like using a calculator or browsing the internet, expanding its problem-solving abilities ([source](https://arxiv.org/abs/2307.16789)).
* **Self-correction**. The model's ability to identify and correct its own mistakes, which is crucial for improving the accuracy of AI-generated content ([source](https://arxiv.org/abs/2303.11366)).



* **Reasoning**. The advancements in LLMs have also led to significant improvements in the ability to process and generate logical chains of thought and reasoning. This is particularly important in problem-solving tasks where a straightforward answer isn't immediately available, and a step-by-step reasoning process is required. ([Source](https://arxiv.org/abs/2303.12712))
* **Programming ability**. In coding, AI models have progressed from basic code autocompletion to writing sophisticated, functional programs.
* **Scientific & Mathematical ability**. In mathematics, AI's have assisted in the subfield of automatic theorem proving for decades. Today's models continue to assist in solving complex problems. AI can even achieve a gold medal level in the mathematical Olympiad by solving geometry problems ([source](https://www.nature.com/articles/s41586-023-06747-5)).



<p id="gdcalert5" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image5.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert6">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image5.png "image_tooltip")



## 
    **Figure**: GPT-4 solves some tasks that GPT-3.5 was unable to, like the uniform bar examination, where GPT-4 scores 90% compared to 10% for GPT-3.5. GPT-4 is also capable of vision processing, and the added vision component had only a minor impact, but it helped others tremendously.  **Source**: [GPT-4](https://openai.com/research/gpt-4)


## Image Generation

The leap forward in image generation is not just in accuracy, but also in the ability to handle complex, real-world images. The latter, particularly with the advent of Generative Adversarial Networks (GANs) in 2014, has shown an astounding rate of progress. The quality of images generated by AI has evolved from simple, blurry representations to highly detailed and creative scenes, often in response to intricate language prompts.


## 
    

<p id="gdcalert6" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image6.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert7">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image6.png "image_tooltip")



## 
    **Figure**: An example of state of the art image recognition. The Segment Anything Model (SAM) by Meta’s FAIR (Fundamental AI Research) lab, can classify and segment visual data at highly precise levels. The detection is performed without the need to annotate images. ([source](https://viso.ai/deep-learning/segment-anything-model-sam-explained/))


<table>
  <tr>
   <td>>
    

<p id="gdcalert7" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image7.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert8">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image7.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert8" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image8.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert9">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image8.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert9" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image9.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert10">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image9.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert10" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image10.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert11">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image10.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>>
    2014
   </td>
   <td>>
    2015
   </td>
   <td>>
    2016
   </td>
   <td>>
    2017
   </td>
  </tr>
  <tr>
   <td>>
    <a href="https://arxiv.org/abs/1406.2661">GANs</a>
   </td>
   <td>>
    <a href="https://arxiv.org/abs/1511.06434">Deep Convolutional GANs</a>
   </td>
   <td>>
    <a href="https://arxiv.org/abs/1606.07536">CoGANs</a>
   </td>
   <td>>
    <a href="https://arxiv.org/abs/1710.10196">Progressive Growing GANs</a>
   </td>
  </tr>
</table>



<table>
  <tr>
   <td>>
    

<p id="gdcalert11" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image11.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert12">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image11.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert12" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image12.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert13">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image12.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert13" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image13.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert14">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image13.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert14" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image14.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert15">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image14.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>>
    2018
   </td>
   <td>>
    2019
   </td>
   <td>>
    2020
   </td>
   <td>>
    2021
   </td>
  </tr>
  <tr>
   <td>>
    <a href="https://arxiv.org/abs/1812.04948">StyleGAN-1</a>
   </td>
   <td>>
    <a href="https://arxiv.org/abs/1912.04958">StyleGAN-2</a>
   </td>
   <td>>
    <a href="https://arxiv.org/abs/2006.11239">Diffusion Models</a>
   </td>
   <td>>
    <a href="https://arxiv.org/abs/2106.12423">StyleGAN-3</a>
   </td>
  </tr>
</table>



<table>
  <tr>
   <td>>
    

<p id="gdcalert15" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image15.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert16">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image15.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert16" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image16.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert17">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image16.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert17" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image17.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert18">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image17.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert18" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image18.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert19">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image18.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td>>
    2021
   </td>
   <td>>
    2022
   </td>
   <td>>
    2022
   </td>
   <td>>
    2023
   </td>
  </tr>
  <tr>
   <td>>
    <a href="https://openai.com/blog/dall-e/">DALL-E</a>
   </td>
   <td>>
    <a href="https://imagen.research.google/">Imagen</a>
   </td>
   <td>>
    <a href="https://openai.com/dall-e-2/">DALL-E 2</a>
   </td>
   <td>>
    <a href="https://en.wikipedia.org/wiki/Midjourney">Midjourney V5</a>
   </td>
  </tr>
</table>



## 
    Figure: An example of evolution of image generation. At the top left, starting from GANs (Generative Adversarial Networks) to the bottom right, an image from MidJourney V5.

The rate of progress within a single year alone is quite astounding as is seen from the improvements between the V1 of the MidJourney image generation model in early 2022, to the V6 in December 2023.


<table>
  <tr>
   <td>

<p id="gdcalert19" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image19.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert20">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image19.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>

<p id="gdcalert20" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image20.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert21">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image20.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>

<p id="gdcalert21" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image21.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert22">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image21.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>

<p id="gdcalert22" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image22.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert23">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image22.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>

<p id="gdcalert23" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image23.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert24">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image23.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>

<p id="gdcalert24" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image24.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert25">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image24.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td> V1 (Feb 22)
   </td>
   <td>V2 (Apr 22)
   </td>
   <td>V3 (Jul 22)
   </td>
   <td>V4 (Nov 22)
   </td>
   <td>V5 (Mar 23)
   </td>
   <td>V6 (Dec 23)
   </td>
  </tr>
  <tr>
   <td colspan="6" >>
    <strong>Figure</strong>: MidJourney AI image generation over 2022-2023.<strong> Prompt</strong>: high quality photography of a young Japanese woman smiling, backlighting, natural pale light, film camera, by Rinko Kawauchi, HDR (<a href="https://goldpenguin.org/blog/midjourney-v1-to-v6-evolution/">source</a>)
   </td>
  </tr>
</table>



## Multi & Cross modality

AI systems are becoming increasingly multimodal. This means that they can process images, text, audio, vision, and robotics using the same model. So they are trained using multiple different “modes” and are able to translate between them after deployment.

**Cross-modality**.  A model is called cross-modal when the input of a model is in one modality (e.g text) and the output is in another modality (e.g. image). The section on computer vision showed fast progress between 2014 and 2020 in cross modality. We went from text-to-image models only capable of generating black-and-white pixelated images of faces, to models capable of generating an image of any textual prompt. More examples of cross-modality include OpenAIs Whisper ([source](https://cdn.openai.com/papers/whisper.pdf)) which is capable speech-to-text transcription.

**Multi-modality**. A model is called multi-modal when both the inputs and outputs of a model can be in more than one modality. E.g. audio-to-text, video-to-text, text-to-image, etc…


<table>
  <tr>
   <td>>
    

<p id="gdcalert25" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image25.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert26">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image25.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert26" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image26.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert27">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image26.png" width="" alt="alt_text" title="image_tooltip">

   </td>
   <td>>
    

<p id="gdcalert27" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image27.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert28">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image27.png" width="" alt="alt_text" title="image_tooltip">

   </td>
  </tr>
  <tr>
   <td colspan="3" >>
    Figure: Image-to-text and text-to-image multimodality from the Flamingo model. (<a href="https://arxiv.org/abs/2204.14198">source</a>)
   </td>
  </tr>
</table>


DeepMind’s 2022 Flamingo model, could be “_rapidly adapted to various image/video understanding tasks_” and “_is also capable of multi-image visual dialogue_”. ([source](https://arxiv.org/abs/2204.14198)) Similarly DeepMind’s 2022 Gato model, was called a "Generalist Agent". It was a single network with the same weights which could “_play Atari, caption images, chat, stack blocks with a real robot arm and much more_”. ([source](https://arxiv.org/abs/2205.06175)) Continuing this trend, DeepMinds 2023 Google's Gemini model could be called a Large Multimodal Models (LMMs). The paper described Gemini as “_natively multimodal_” and claimed to be able to “_seamlessly combine their capabilities across modalities (e.g. extracting information and spatial layout out of a table, a chart, or a figure) with the strong reasoning capabilities of a language model (e.g. its state-of-art-performance in math and coding)_”([source](https://arxiv.org/abs/2312.11805))




## Robotics

The field of robotics has also been progressing alongside artificial intelligence. In this section we provide a couple of examples where these two fields are merging, highlighting some robots using inspiration from machine learning techniques to make advancements.


##### 
    

<p id="gdcalert28" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image28.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert29">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image28.png "image_tooltip")



## 
    **Figure**: Researchers used Model-Free Reinforcement Learning to automatically learn quadruped locomotion in only 20 minutes in the real world instead of in simulated environments. The Figure shows examples of learned gaits on a variety of real-world terrains. ([source](https://arxiv.org/abs/2208.07860))

**Advances in robotics**. At the forefront of robotic advancements is PaLM-E, a general-purpose, embodied model with 562 billion parameters that integrates vision, language, and robot data for real-time manipulator control and excels in language tasks involving geospatial reasoning. ([source](https://arxiv.org/abs/2303.03378))

Simultaneously, developments in vision-language models have led to breakthroughs in fine grained robot control, with models like RT-2 showing significant capabilities in object manipulation and multimodal reasoning. RT-2 demonstrates how we can use LLM inspired prompting methods (chain-of-thought), to  learn a self-contained model that can both plan long-horizon skill sequences and predict robot actions. ([source](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/))

Mobile ALOHA is another example of combining modern machine learning techniques with robotics. Trained using supervised behavioral cloning the robot can autonomously perform complex tasks “_such as sauteing and serving a piece of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling and entering an elevator, and lightly rinsing a used pan using a kitchen faucet._” ([source](https://arxiv.org/abs/2401.02117)) Such advancements not only demonstrate the increasing sophistication and applicability of robotic systems but also highlight the potential for further groundbreaking developments in autonomous technologies.


##### 
    

<p id="gdcalert29" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image29.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert30">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image29.png "image_tooltip")



## 
    Figure: DeepMinds RT-2 can both plan long-horizon skill sequences and predict robot actions using inspiration from LLM prompting techniques (chain-of-thought). ([source](https://deepmind.google/discover/blog/rt-2-new-model-translates-vision-and-language-into-action/))




## Gaming

**AI and board games.** AI has made continuous progress in game playing for decades. Starting from AIs beating the world champion at chess in 1997 ([source](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer))), Scrabble in 2006 ([source](https://content.time.com/time/specials/packages/article/0,28804,2049187_2049195_2049083,00.html)) to DeepMinds [AlphaGo](https://www.deepmind.com/research/highlighted-research/alphago) in 2016, which was good enough to defeat the world champion in the game of Go, a game assumed to be notoriously difficult for AI. Within a year the next model [AlphaZero](https://www.deepmind.com/blog/alphazero-shedding-new-light-on-chess-shogi-and-go) trained through self-play had mastered multiple games of Go, chess, and shogi reaching a superhuman level after less than three days of training.

**AI and video games.** We started using machine learning techniques on simple Atari games in 2013 ([source](https://arxiv.org/abs/1312.5602)). By 2019, OpenAI Five defeated the world champions at DOTA2 ([source](https://openai.com/research/openai-five-defeats-dota-2-world-champions)), while in the same year, DeepMinds AlphaStar beat professional esports players at StarCraft II ([source](https://www.deepmind.com/blog/alphastar-mastering-the-real-time-strategy-game-starcraft-ii)). Both these games require thousands of actions in a row at a high number of actions per minute. In 2020 DeepMind MuZero model, described as “_a significant step forward in the pursuit of general-purpose algorithms_” ([source](https://www.deepmind.com/blog/muzero-mastering-go-chess-shogi-and-atari-without-rules)), was capable of playing Atari games, Go, chess, and shogi without even being told the rules.

In recent years, AI's capability has extended to open-ended environments like Minecraft, showcasing an ability to perform complex sequences of actions. In strategy games, Meta’s Cicero displayed intricate strategic and negotiation and deception skills in natural language for the game Diplomacy ([source](https://arxiv.org/abs/2210.05492)).


## 
    

<p id="gdcalert30" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image30.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert31">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image30.png "image_tooltip")
 \
**Figure**: A map of diplomacy and the dialog box where the AI negotiates. ([source](https://www.youtube.com/watch?v=u5192bvUS7k&t=2216s))


<table>
  <tr>
   <td><strong>Box. Example of Voyager: Planning and Continuous Learning in Minecraft with GPT-4</strong>
   </td>
  </tr>
  <tr>
   <td>Voyager (<a href="https://arxiv.org/abs/2305.16291">source</a>) stands as a particularly impressive example of the capabilities of AI in continuous learning environments. This AI is designed to play Minecraft, a task that involves a significant degree of planning and adaptive learning. What makes Voyager so remarkable is its ability to learn continuously and progressively within the game's environment, using GPT-4 contextual reasoning abilities to plan and write the code necessary for each new challenge. Starting from scratch in a single game session, Voyager initially learns to navigate the virtual world, engage and defeat enemies, and remember all these skills in its long-term memory. As the game progresses, it continues to learn and store new skills, leading up to the challenging task of mining diamonds, a complex activity that requires a deep understanding of the game mechanics and strategic planning. The ability of Voyager to integrate new information continuously and utilize it effectively showcases the potential of AI in managing complex, changing environments and performing tasks that require a long-term buildup of knowledge and skills.>
    

<p id="gdcalert31" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image31.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert32">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


<img src="images/image31.png" width="" alt="alt_text" title="image_tooltip">
>
    Figure from Voyager: Voyager discovers new Minecraft items and skills continually by self-driven exploration, significantly outperforming the baselines.
   </td>
  </tr>
</table>



# Foundation Models

Foundation models represent a significant shift in artificial intelligence, marking a broader change in our approach to machine learning. This evolution reflects our journey from an initial reliance on expert-crafted features to leveraging massive, diverse datasets that allow AI systems to auto learn and adapt across a wide range of tasks. Let’s explore how this progression unfolded building up to an understanding of the foundation model paradigm.

**Hand-Engineered Features**. In the early days of AI, the field focused on using expert knowledge to manually design features that algorithms could learn from. This era, characterized by deep domain knowledge and meticulous detail, laid the groundwork for AI but also showcased its limitations. Each new task required a fresh set of features, posing scalability challenges and highlighting the need for a more adaptable approach.

**Deep Learning**. The transition to deep learning, around 2010, marked a pivotal shift. Deep learning enabled computers to autonomously learn complex features and patterns directly from data, bypassing the need for human-guided feature design. This advancement significantly reduced the reliance on human expertise, paving the way for more universal and adaptable AI systems. As deep learning evolved, it led to an explosion of specialized models trained for distinct tasks, demonstrating the technology's vast potential across various domains.

**Foundation Models: The Swiss Army Knives of AI. **Foundation models emerged in the mid-to-late 2010s, symbolizing a move away from the labor-intensive, one-model-per-task approach. These advanced AI systems are trained on vast, diverse datasets to learn broad patterns and skills, ready to be adapted to a multitude of tasks. Imagine them as the Swiss Army knives of the AI world: versatile tools that, with the right adjustments, can tackle everything from language translation to generating artwork. This paradigm shift towards foundation models introduced a more efficient strategy, leveraging large, unlabeled datasets to create models that can later be fine-tuned for specific needs.

The term "foundation" aptly describes these models as the groundwork for more refined, task-specific applications. Like the foundation of a building, their reliability and strength are crucial. The versatility designed into these models means their performance in specific applications can be unpredictable, raising challenges in ensuring their stability, safety, and security. Yet, this approach also offers significant time and resource savings, highlighting the trade-offs involved in adopting foundation models.

**Economics of Foundation Models. **This monumental shift was fueled by several factors: the explosion of data, advances in computational power, and refinements in machine learning techniques. But it’s important to understand the economic backdrop that has shaped the development of foundation models. These models are complex and resource-intensive, with their development, training, and deployment often requiring significant investment. The development of foundation models involves substantial costs, categorized into three main areas:



* **Data Acquisition**. The backbone of foundation models is the large-scale datasets they're trained on, often sourced from the internet. Collecting, cleaning, and updating these datasets can be expensive, especially for specialized or proprietary data.
* **Computational Resources**. The sheer size of foundation models and the datasets used in their training demand significant computational resources, not just in terms of hardware but also the electricity needed for operation.
* **Research and Development**. Beyond the immediate costs of data and computation, there's the ongoing investment in research required to develop, fine-tune, and ensure the safe use of foundation models. This encompasses not just researching improvements, but also staying up to date on the models' societal impacts and mitigation potential risks. All this necessitates significant financial resources and specialized expertise.

The next section provides a deeper dive into the machinery that powers these AI giants. The breakthroughs in pre-training, transfer learning, self-supervised learning, few-shot learning, and fine-tuning are what drive the versatility and effectiveness of foundation models. Understanding these underlying techniques is crucial for appreciating how foundation models work.


## Techniques

**Pre-training: Starting school.** Imagine being a student in the early years of education. In school, you are exposed to a wide range of subjects—math, science, history, language arts, and more. This broad education is akin to pre-training in machine learning. It's a learning phase which equips them with a broad base of knowledge, preparing them for more specialized studies later.

Pre-training is essentially the boot camp for machine learning models, where they undergo initial training on a large dataset comprising millions, if not billions, of examples. Here they learn general patterns, structures, and knowledge which will be invaluable when it moves on to tackle more specific challenges. Older more traditional approaches trained models from scratch for every new task, which was not only resource-intensive but also limited in scope. By embracing pre-training, models can now draw upon this extensive period of learning to significantly enhance their performance on a wide range of tasks later on. The role of pre-training cannot be overstated—it is the bedrock upon which foundation models are constructed. By endowing models with a wide-ranging understanding of data, they are poised to approach specific tasks with heightened efficiency and precision.

**Self-Supervised Learning: Independent Study Projects.** The same student might need to undertake an independent study project, where they choose a topic of interest and explore it deeply without much direct instruction, relying on their initiative to research, synthesize information, and draw conclusions. This mirrors self-supervised learning in machine learning, where models learn from the data itself, finding structure and patterns without explicit labels or guidance from teachers.

SSL is how we actually implement the pre-training phase for foundation models. Unlike traditional supervised learning that relies heavily on labeled data, Self-Supervised Learning (SSL) leverages unlabeled data, enabling models to learn from the inherent structure of the data itself. The development of this technique was a crucial step because it allowed developers to not be restricted by human labels. Now, we can leverage nearly unlimited (unlabeled) data available on the web.

As an example of how this technique would work - suppose you have an image of a dog in a park. Instead of a human labeling the image, and then training the model to learn what the human would say, the task for the model is to predict a portion of the image given the rest of it. For instance, the model might be given the top half of the image, and its task would be to predict what the bottom half looks like. 

This is repeated on a large number of such images, learning to recognize patterns and structures in this data. Through these examples, the model might learn for instance that images with trees and grass at the top often have more grass, or perhaps a path, at the bottom. It learns about objects and their context — trees and grass often appear in parks, dogs are often found in these environments, paths are usually horizontal, and so on. These learned representations can then be used for a wide variety of tasks that the model was not explicitly trained for, such as identifying dogs in images, or recognizing parks - all without any human provided labels!

**Zero & Few-Shot Learning: Adapting to Unfamiliar Questions on Tests**. Imagine the student encountering a question on an exam they've never explicitly prepared for. They must harness their broader knowledge and reasoning skills to construct an educated guess.

Zero or few-shot learning are techniques in machine learning where models learn to perform tasks with very few examples. Zero shot is when they perform well without any specific examples at all. This is yet another example of a technique which is useful when collecting extensive labeled data is impractical or too costly. To grasp few-shot learning, picture introducing a student to the concept of a cat for the first time with just a few images. Despite only seeing three examples, the student learns to identify cats in a variety of contexts, not limited to the initial examples. Similarly, few-shot learning enables AI models to generalize from a minimal set of instances, identifying new examples in broader categories they've scarcely encountered.

These techniques stand out for their potential to mimic human-like learning efficiency, achieving comprehension and application with minimal examples. In the realm of foundation models like GPT-3, extensive pre-training on diverse datasets equips these models with a broad understanding of language and context. This foundational knowledge is then applied in few-shot or zero-shot scenarios, where the model is prompted to apply its vast background to specific, narrowly defined tasks. While foundation models have demonstrated impressive capabilities in few-shot learning, their proficiency in zero-shot scenarios remains a work in progress, but is expected to keep improving over the upcoming years.

**Transfer Learning: Applying Skills to a New Subject.** Imagine a student who has been absorbing knowledge across a broad spectrum of subjects. Now as they progress into high school, they decide to take on a new challenge, like an elective in computer science, a field they've never formally studied. But they have basic skills in critical thinking, research, and analysis learned from other subjects which they can use to grasp concepts more effectively. This scenario mirrors transfer learning in AI, where a model, supported by the pre-training phase, is adapted to excel in a new, related task.

Transfer learning is the subsequent step that follows the pre-training of a model. It's where the model takes the general patterns, structures, and knowledge it has gleaned from the pre-training phase and applies them to new, related tasks. This technique hinges on the premise that knowledge acquired in one context can actually be "transferred" to enhance learning in another. It allows for the utilization of pre-existing knowledge, thereby sidestepping the need to start from scratch for every new task. This approach proves especially invaluable in scenarios where labeled data for the new task are scarce or hard to come by.  Just like pre-training, transfer learning is indispensable in maximizing the utility and adaptability of foundation models. It enables these extensively pre-trained models to apply their broad base of knowledge to a wide array of specialized tasks.

**Fine-Tuning: Specializing in a College Major**. The student now embarks on their college journey, choosing a major to specialize in, such as biology. This decision leads them to delve deeper into this field through advanced courses, honing their expertise in one particular domain. This educational path is akin to the process of fine-tuning in machine learning, where a pre-trained model undergoes further training on a more focused dataset. This additional phase refines the model's abilities, optimizing its performance for specific tasks within its broader knowledge base.

While pre-training provides a model with a comprehensive knowledge base, it was only part of the journey. The subsequent fine-tuning phase is where the model is specifically adapted to perform particular tasks. This two-step approach—starting with pre-training and followed by fine-tuning—endows foundation models with their remarkable versatility and power.

Fine-tuning enables the creation of versatile models capable of undertaking a wide array of tasks, from adhering to specific instructions in a conversational model to excelling in niche areas like programming or scientific analysis. This adaptability is further enhanced through methods like "Reinforcement Learning from Human Feedback" (RLHF), which refines models to be more effective and user-friendly by reinforcing desirable outputs. Such techniques will be discussed in detail in later chapters.



<p id="gdcalert32" ><span style="color: red; font-weight: bold">>>>>>  gd2md-html alert: inline image link here (to images/image32.png). Store image on your image server and adjust path/filename/extension if necessary. </span><br>(<a href="#">Back to top</a>)(<a href="#gdcalert33">Next alert</a>)<br><span style="color: red; font-weight: bold">>>>>> </span></p>


![alt_text](images/image32.png "image_tooltip")


Source: Bommasani Rishi et. al. (2022) "[On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)"

**Elicitation Techniques: Prompting for Specific Insights**. Finally, as the student works on their thesis or a capstone project, they might frequently consult with their professor or mentor, asking specific questions or seeking advice on their research direction. This interaction is similar to prompting in AI, where a trained model is given a specific query or "prompt" to generate a targeted response or output, leveraging its trained knowledge base to provide relevant information or extremely specific insights.

Prompting serves as a critical interface between human users and the sophisticated capabilities of foundation models. It's akin to giving the model a nudge in the right direction, ensuring that the vast knowledge it has acquired is applied in a way that's relevant and useful. The development of prompting techniques also reflects a broader shift in AI towards more interactive, user-friendly models like ChatGPT. We only briefly introduce the concept here, a broad variety of elicitation and prompting techniques will be discussed in later chapters.

The learning journey of a student—beginning with acquiring broad knowledge, honing specific skills, engaging in self-directed exploration, and seeking expert guidance—mirrors the trajectory of foundation model development. From the foundational broad learning in pre-training through specialization via fine-tuning to the targeted application through prompting, this progression encapsulates the evolving interaction between AI models and the complex, multifaceted tasks they are now capable of addressing.
